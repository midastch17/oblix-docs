{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Oblix SDK","text":"<ul> <li> <p> Seamless AI Orchestration</p> <p>Intelligently route between local and cloud models based on connectivity, resources, and cost.</p> <p> Get started</p> </li> <li> <p> Model Switching</p> <p>Automatically switch between local Ollama models and cloud APIs like OpenAI and Claude.</p> <p> Model providers</p> </li> <li> <p> Connectivity Aware</p> <p>Gracefully handle offline scenarios and degraded connections with intelligent fallbacks.</p> <p> How it works</p> </li> <li> <p> Resource Optimized</p> <p>Route based on system resources to ensure optimal performance in any environment.</p> <p> Resource monitoring</p> </li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>```bash pip install oblix</p>"},{"location":"api-reference/","title":"API Reference","text":"<p>This section provides detailed documentation of the Oblix SDK API.</p>"},{"location":"api-reference/#overview","title":"Overview","text":"<p>Oblix provides a simple, unified API to orchestrate AI models across different providers (OpenAI, Claude, Ollama) with intelligent routing based on system resources and connectivity. The API is designed to be intuitive and developer-friendly while providing powerful capabilities.</p>"},{"location":"api-reference/#main-components","title":"Main Components","text":""},{"location":"api-reference/#oblixclient","title":"OblixClient","text":"<p>The main entry point to the SDK. Handles model management, execution routing, and session management.</p> <pre><code>from oblix import OblixClient\n\n# Initialize with your API key\nclient = OblixClient(oblix_api_key=\"your_api_key\")\n</code></pre>"},{"location":"api-reference/#modeltype","title":"ModelType","text":"<p>Enum that defines the supported model types/providers.</p> <pre><code>from oblix import ModelType\n\n# Available model types\nModelType.OLLAMA  # Local models via Ollama\nModelType.OPENAI  # OpenAI models (GPT series)\nModelType.CLAUDE  # Anthropic Claude models\n</code></pre>"},{"location":"api-reference/#agents","title":"Agents","text":"<p>Monitoring agents that provide system awareness for intelligent routing.</p> <pre><code>from oblix.agents import ResourceMonitor, ConnectivityAgent\n\n# Add monitoring capabilities\nclient.hook_agent(ResourceMonitor())\nclient.hook_agent(ConnectivityAgent())\n</code></pre>"},{"location":"api-reference/#basic-workflow","title":"Basic Workflow","text":"<p>Using the Oblix API typically follows this pattern:</p> <ol> <li>Initialize client</li> <li>Hook models (at least one local and one cloud model)</li> <li>Add monitoring agents</li> <li>Execute prompts or start chat sessions</li> </ol> <pre><code>import asyncio\nfrom oblix import OblixClient, ModelType\nfrom oblix.agents import ResourceMonitor, ConnectivityAgent\n\nasync def main():\n    # 1. Initialize client\n    client = OblixClient(oblix_api_key=\"your_api_key\")\n\n    # 2. Hook models\n    await client.hook_model(ModelType.OLLAMA, \"llama2\")\n    await client.hook_model(ModelType.OPENAI, \"gpt-3.5-turbo\", api_key=\"your_openai_api_key\")\n\n    # 3. Add monitoring agents\n    client.hook_agent(ResourceMonitor())\n    client.hook_agent(ConnectivityAgent())\n\n    # 4. Execute a prompt\n    response = await client.execute(\"Explain quantum computing in simple terms\")\n    print(response[\"response\"])\n\n# Run the example\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"api-reference/#next-steps","title":"Next Steps","text":"<p>Explore detailed documentation for each component:</p> <ul> <li>OblixClient API Reference</li> <li>ModelType Enum Reference</li> <li>Agents Reference</li> </ul> <p>This index page gives a clear overview of the Oblix API, introduces the main components, and provides a basic workflow example. It also includes links to more detailed documentation for each API component.</p>"},{"location":"api-reference/agents/","title":"Agents","text":"<p>Agents in Oblix are monitoring components that provide system awareness and make intelligent routing decisions. They analyze system resources, network connectivity, and other factors to help the SDK choose the optimal model for execution.</p>"},{"location":"api-reference/agents/#available-agents","title":"Available Agents","text":"<p>Oblix provides two main types of monitoring agents:</p>"},{"location":"api-reference/agents/#resourcemonitor","title":"ResourceMonitor","text":"<p>Monitors system resources including CPU, memory, and GPU utilization.</p> <pre><code>from oblix.agents import ResourceMonitor\n\n# Create with default settings\nresource_monitor = ResourceMonitor()\n\n# Create with custom thresholds\nresource_monitor = ResourceMonitor(\n    name=\"custom_resource_monitor\",\n    custom_thresholds={\n        \"cpu_threshold\": 70.0,     # CPU usage percentage (default: 80.0)\n        \"memory_threshold\": 75.0,  # RAM usage percentage (default: 85.0)\n        \"load_threshold\": 3.0,     # System load average (default: 4.0)\n        \"gpu_threshold\": 75.0      # GPU utilization percentage (default: 85.0)\n    }\n)\n</code></pre>"},{"location":"api-reference/agents/#resource-states","title":"Resource States","text":"<p>The resource monitor reports one of the following states:</p> State Description <code>AVAILABLE</code> Sufficient resources available for local execution <code>CONSTRAINED</code> Resources are limited but usable <code>CRITICAL</code> Resources are extremely limited, recommend cloud execution"},{"location":"api-reference/agents/#execution-targets","title":"Execution Targets","text":"<p>Based on resource state, the monitor recommends one of these execution targets:</p> Target Description <code>LOCAL_CPU</code> Execute on local CPU <code>LOCAL_GPU</code> Execute on local GPU (if available and supported) <code>CLOUD</code> Execute on cloud model"},{"location":"api-reference/agents/#connectivityagent","title":"ConnectivityAgent","text":"<p>Monitors network connectivity, including latency, packet loss, and bandwidth.</p> <pre><code>from oblix.agents import ConnectivityAgent\n\n# Create with default settings\nconnectivity_agent = ConnectivityAgent()\n\n# Create with custom configuration\nconnectivity_agent = ConnectivityAgent(\n    name=\"custom_connectivity_monitor\",\n    latency_threshold=150.0,       # Maximum acceptable latency in ms (default: 200.0)\n    packet_loss_threshold=5.0,     # Maximum acceptable packet loss percentage (default: 10.0)\n    bandwidth_threshold=10.0,      # Minimum acceptable bandwidth in Mbps (default: 5.0)\n    check_interval=60              # Seconds between connectivity checks (default: 30)\n)\n</code></pre>"},{"location":"api-reference/agents/#connectivity-states","title":"Connectivity States","text":"<p>The connectivity agent reports one of the following states:</p> State Description <code>OPTIMAL</code> Good connectivity suitable for cloud model use <code>DEGRADED</code> Limited connectivity that may affect performance <code>DISCONNECTED</code> No connectivity, must use local models"},{"location":"api-reference/agents/#connection-targets","title":"Connection Targets","text":"<p>Based on connectivity state, the agent recommends one of these targets:</p> Target Description <code>LOCAL</code> Use local models due to connectivity issues <code>CLOUD</code> Use cloud models due to good connectivity <code>HYBRID</code> Consider balancing between local and cloud models"},{"location":"api-reference/agents/#using-agents","title":"Using Agents","text":"<p>Agents are added to the Oblix client with the <code>hook_agent()</code> method:</p> <pre><code>from oblix import OblixClient\nfrom oblix.agents import ResourceMonitor, ConnectivityAgent\n\n# Initialize client\nclient = OblixClient(oblix_api_key=\"your_api_key\")\n\n# Add monitoring agents\nclient.hook_agent(ResourceMonitor())\nclient.hook_agent(ConnectivityAgent())\n</code></pre> <p>Once agents are hooked, the Oblix client automatically consults them during execution to make intelligent routing decisions.</p>"},{"location":"api-reference/agents/#agent-checks","title":"Agent Checks","text":"<p>When you execute a prompt using the <code>execute()</code> method, Oblix runs checks with all registered agents. The results of these checks are included in the response under the <code>agent_checks</code> key:</p> <pre><code>response = await client.execute(\"Explain quantum computing\")\n\n# Access agent check results\nagent_checks = response[\"agent_checks\"]\n</code></pre> <p>The <code>agent_checks</code> dictionary contains information about:</p> <ul> <li>Current resource state</li> <li>Connectivity state</li> <li>Recommended execution target</li> <li>Detailed metrics</li> <li>Reasoning for the recommendation</li> </ul>"},{"location":"api-reference/model-types/","title":"ModelType","text":"<p>The <code>ModelType</code> enum defines the supported AI model providers in the Oblix system. It's used when hooking models to specify which provider the model belongs to.</p>"},{"location":"api-reference/model-types/#import","title":"Import","text":"<pre><code>from oblix import ModelType\n</code></pre>"},{"location":"api-reference/model-types/#enum-values","title":"Enum Values","text":"Enum Value Description <code>ModelType.OLLAMA</code> Local models served via Ollama <code>ModelType.OPENAI</code> Cloud models provided by OpenAI (GPT series) <code>ModelType.CLAUDE</code> Cloud models provided by Anthropic (Claude series) <code>ModelType.CUSTOM</code> Custom model implementations"},{"location":"api-reference/model-types/#usage","title":"Usage","text":"<p>The <code>ModelType</code> enum is used primarily with the <code>hook_model()</code> method to specify which provider a model belongs to:</p> <pre><code>from oblix import OblixClient, ModelType\n\nclient = OblixClient(oblix_api_key=\"your_api_key\")\n\n# Hook an Ollama model\nawait client.hook_model(\n    model_type=ModelType.OLLAMA,\n    model_name=\"llama2\"\n)\n\n# Hook an OpenAI model\nawait client.hook_model(\n    model_type=ModelType.OPENAI,\n    model_name=\"gpt-3.5-turbo\",\n    api_key=\"your_openai_api_key\"\n)\n\n# Hook a Claude model\nawait client.hook_model(\n    model_type=ModelType.CLAUDE,\n    model_name=\"claude-3-opus-20240229\",\n    api_key=\"your_anthropic_api_key\"\n)\n</code></pre>"},{"location":"api-reference/model-types/#required-parameters-by-type","title":"Required Parameters by Type","text":"<p>Each model type requires specific parameters:</p>"},{"location":"api-reference/model-types/#ollama","title":"OLLAMA","text":"<ul> <li><code>model_name</code>: Name of the Ollama model (e.g., \"llama2\")</li> <li><code>endpoint</code>: (Optional) API endpoint (default: \"http://localhost:11434\")</li> </ul>"},{"location":"api-reference/model-types/#openai","title":"OPENAI","text":"<ul> <li><code>model_name</code>: Name of the OpenAI model (e.g., \"gpt-3.5-turbo\")</li> <li><code>api_key</code>: Your OpenAI API key</li> </ul>"},{"location":"api-reference/model-types/#claude","title":"CLAUDE","text":"<ul> <li><code>model_name</code>: Name of the Claude model (e.g., \"claude-3-opus-20240229\")</li> <li><code>api_key</code>: Your Anthropic API key</li> </ul>"},{"location":"api-reference/model-types/#custom","title":"CUSTOM","text":"<ul> <li><code>model_name</code>: Name of your custom model</li> <li>Additional parameters as required by your implementation</li> </ul>"},{"location":"api-reference/model-types/#supported-models","title":"Supported Models","text":""},{"location":"api-reference/model-types/#ollama_1","title":"OLLAMA","text":"<p>Ollama supports various open-source models, including:</p> <ul> <li><code>llama2</code></li> <li><code>mixtral</code></li> <li><code>mistral</code></li> <li><code>phi</code></li> <li><code>gemma</code></li> <li>And many others available through Ollama</li> </ul>"},{"location":"api-reference/model-types/#openai_1","title":"OPENAI","text":"<p>Supported OpenAI models include:</p> <ul> <li><code>gpt-4-turbo</code></li> <li><code>gpt-4</code></li> <li><code>gpt-3.5-turbo</code></li> <li>And other GPT models available through the OpenAI API</li> </ul>"},{"location":"api-reference/model-types/#claude_1","title":"CLAUDE","text":"<p>Supported Claude models include:</p> <ul> <li><code>claude-3-opus-20240229</code></li> <li><code>claude-3-sonnet-20240229</code></li> <li><code>claude-3-haiku-20240307</code></li> <li>And other Claude models available through the Anthropic API</li> </ul>"},{"location":"api-reference/oblix-client/","title":"OblixClient API Reference","text":"<p>The <code>OblixClient</code> class is the main entry point for the Oblix SDK. It provides a high-level interface for working with multiple AI models, managing sessions, and utilizing intelligent orchestration based on system resources and connectivity.</p>"},{"location":"api-reference/oblix-client/#initialization","title":"Initialization","text":"<pre><code>from oblix import OblixClient\n\nclient = OblixClient(\n    oblix_api_key: str = None,  # Oblix API key (or use OBLIX_API_KEY env var)\n    host: str = \"localhost\",     # Host address (default: localhost)\n    port: int = 4321,           # Port number (default: 4321)\n    config_path: str = None     # Path to configuration file (optional)\n)\n</code></pre>"},{"location":"api-reference/oblix-client/#core-methods","title":"Core Methods","text":""},{"location":"api-reference/oblix-client/#hook-model","title":"Hook Model","text":"<p>Register a new AI model with the client.</p> <pre><code>async def hook_model(\n    model_type: ModelType,      # Type of model (e.g., ModelType.OLLAMA)\n    model_name: str,            # Name of model (e.g., \"llama2\", \"gpt-3.5-turbo\") \n    endpoint: str = None,       # API endpoint for Ollama models (optional)\n    api_key: str = None,        # API key for cloud models (optional)\n    **kwargs                    # Additional model-specific parameters\n) -&gt; bool:                      # Returns True if successful\n</code></pre> <p>Example: <pre><code># Hook an Ollama model\nawait client.hook_model(\n    model_type=ModelType.OLLAMA,\n    model_name=\"llama2\",\n    endpoint=\"http://localhost:11434\"\n)\n\n# Hook an OpenAI model\nawait client.hook_model(\n    model_type=ModelType.OPENAI,\n    model_name=\"gpt-3.5-turbo\",\n    api_key=\"your_openai_api_key\"\n)\n\n# Hook a Claude model\nawait client.hook_model(\n    model_type=ModelType.CLAUDE,\n    model_name=\"claude-3-opus-20240229\",\n    api_key=\"your_anthropic_api_key\"\n)\n</code></pre></p>"},{"location":"api-reference/oblix-client/#hook-agent","title":"Hook Agent","text":"<p>Register an agent with the client for monitoring and intelligent orchestration.</p> <pre><code>def hook_agent(\n    agent: BaseAgent           # Agent instance to hook\n) -&gt; bool:                     # Returns True if successful\n</code></pre> <p>Example: <pre><code>from oblix.agents import ResourceMonitor, ConnectivityAgent\n\n# Add resource monitoring\nclient.hook_agent(ResourceMonitor())\n\n# Add connectivity monitoring\nclient.hook_agent(ConnectivityAgent())\n</code></pre></p>"},{"location":"api-reference/oblix-client/#execute","title":"Execute","text":"<p>Execute a prompt using available models with intelligent orchestration.</p> <pre><code>async def execute(\n    prompt: str,               # User prompt to process\n    model_id: str = None,      # Specific model to use (optional)\n    temperature: float = None, # Sampling temperature (optional)\n    max_tokens: int = None,    # Maximum tokens to generate (optional)\n    request_id: str = None,    # Custom request identifier (optional)\n    **kwargs                   # Additional model-specific parameters\n) -&gt; dict:                     # Response dictionary\n</code></pre> <p>Response structure: <pre><code>{\n    \"request_id\": str,         # Request identifier\n    \"model_id\": str,           # ID of model used for generation\n    \"response\": str,           # Generated text response\n    \"metrics\": dict,           # Performance metrics\n    \"agent_checks\": dict       # Results from agent checks\n}\n</code></pre></p> <p>Example: <pre><code># Basic execution\nresponse = await client.execute(\"Explain quantum computing\")\nprint(response[\"response\"])\n\n# Execution with specific model and parameters\nresponse = await client.execute(\n    \"Write a story about robots\",\n    model_id=\"openai:gpt-3.5-turbo\",\n    temperature=0.8,\n    max_tokens=500\n)\n</code></pre></p>"},{"location":"api-reference/oblix-client/#execute-streaming","title":"Execute Streaming","text":"<p>Execute a prompt with streaming output directly to the terminal.</p> <pre><code>async def execute_streaming(\n    prompt: str,               # User prompt to process\n    model_id: str = None,      # Specific model to use (optional)\n    temperature: float = None, # Sampling temperature (optional)\n    max_tokens: int = None,    # Maximum tokens to generate (optional)\n    request_id: str = None,    # Custom request identifier (optional)\n    **kwargs                   # Additional model-specific parameters\n) -&gt; dict:                     # Final response dictionary\n</code></pre> <p>Example: <pre><code># Stream a response\nresponse = await client.execute_streaming(\"Explain quantum computing\")\n# Tokens are printed to the console in real-time, then final response returned\n</code></pre></p>"},{"location":"api-reference/oblix-client/#chat","title":"Chat","text":"<p>Send a single message in a chat session with automatic session handling.</p> <pre><code>async def chat_once(\n    prompt: str,               # User message\n    session_id: str = None     # Existing session ID (optional)\n) -&gt; dict:                     # Response dictionary including session_id\n</code></pre> <p>Example: <pre><code># Send a message (creates a new session)\nresponse = await client.chat_once(\"Hello, how are you?\")\nsession_id = response[\"session_id\"]\n\n# Continue the conversation\nresponse = await client.chat_once(\"Tell me about yourself\", session_id)\n</code></pre></p>"},{"location":"api-reference/oblix-client/#start-interactive-chat","title":"Start Interactive Chat","text":"<p>Start an interactive chat session in the terminal.</p> <pre><code>async def start_chat(\n    session_id: str = None     # Optional existing session ID to resume\n) -&gt; str:                      # Session ID for the current chat\n</code></pre> <p>Example: <pre><code># Start a new interactive chat session\nawait client.start_chat()\n\n# Resume an existing session\nawait client.start_chat(\"5f3e9a7b-6c1d-4d8e-9e7a-9b8c7d6e5f4a\")\n</code></pre></p>"},{"location":"api-reference/oblix-client/#start-interactive-chat-with-streaming","title":"Start Interactive Chat with Streaming","text":"<p>Start an interactive chat session with streaming responses.</p> <pre><code>async def chat_streaming(\n    session_id: str = None     # Optional existing session ID to resume\n) -&gt; str:                      # Session ID for the current chat\n</code></pre> <p>Example: <pre><code># Start a new interactive streaming chat session\nawait client.chat_streaming()\n\n# Resume an existing session with streaming\nawait client.chat_streaming(\"5f3e9a7b-6c1d-4d8e-9e7a-9b8c7d6e5f4a\")\n</code></pre></p>"},{"location":"api-reference/oblix-client/#session-management-methods","title":"Session Management Methods","text":""},{"location":"api-reference/oblix-client/#create-session","title":"Create Session","text":"<p>Create a new chat session.</p> <pre><code>async def create_session(\n    title: str = None,         # Optional session title\n    initial_context: dict = None # Optional initial context\n) -&gt; str:                      # New session ID\n</code></pre>"},{"location":"api-reference/oblix-client/#list-sessions","title":"List Sessions","text":"<p>List recent chat sessions.</p> <pre><code>def list_sessions(\n    limit: int = 50           # Maximum number of sessions to return\n) -&gt; list:                    # List of session metadata dictionaries\n</code></pre>"},{"location":"api-reference/oblix-client/#load-session","title":"Load Session","text":"<p>Load a specific chat session by ID.</p> <pre><code>def load_session(\n    session_id: str           # Session identifier\n) -&gt; dict:                    # Session data if found, None otherwise\n</code></pre>"},{"location":"api-reference/oblix-client/#delete-session","title":"Delete Session","text":"<p>Delete a chat session permanently.</p> <pre><code>def delete_session(\n    session_id: str           # Session identifier\n) -&gt; bool:                    # True if deleted successfully\n</code></pre>"},{"location":"api-reference/oblix-client/#utility-methods","title":"Utility Methods","text":""},{"location":"api-reference/oblix-client/#list-models","title":"List Models","text":"<p>List all available models grouped by type.</p> <pre><code>def list_models() -&gt; dict:    # Dictionary mapping model types to lists of model names\n</code></pre>"},{"location":"api-reference/oblix-client/#get-model","title":"Get Model","text":"<p>Get configuration for a specific model.</p> <pre><code>def get_model(\n    model_type: str,          # Type of model (e.g., 'ollama', 'openai', 'claude')\n    model_name: str           # Name of model\n) -&gt; dict:                    # Model configuration if found\n</code></pre>"},{"location":"api-reference/oblix-client/#get-connectivity-metrics","title":"Get Connectivity Metrics","text":"<p>Get current connectivity metrics from the connectivity monitor.</p> <pre><code>async def get_connectivity_metrics() -&gt; dict:  # Dictionary of connectivity metrics\n</code></pre>"},{"location":"api-reference/oblix-client/#shutdown","title":"Shutdown","text":"<p>Gracefully shut down all models and agents.</p> <pre><code>async def shutdown() -&gt; None:  # Clean up all resources\n</code></pre>"},{"location":"cli/","title":"CLI Reference","text":"<p>Oblix provides a powerful command-line interface (CLI) that enables you to interact with the Oblix AI Orchestration SDK directly from your terminal. This tool simplifies model management, agent integration, and testing.</p>"},{"location":"cli/#installation","title":"Installation","text":"<p>The Oblix CLI is included with the main Oblix SDK package when you download and install it from oblix.ai.</p> <p>Note: Currently, Oblix only supports macOS. Windows and Linux versions are planned for future releases.</p>"},{"location":"cli/#basic-usage","title":"Basic Usage","text":"<p>Once installed, you can access the CLI by using the <code>oblix</code> command in your terminal:</p> <pre><code>oblix --help\n</code></pre> <p>This will display the available commands and options.</p>"},{"location":"cli/#command-structure","title":"Command Structure","text":"<p>The Oblix CLI follows this general structure:</p> <pre><code>oblix [command] [subcommand] [options]\n</code></pre>"},{"location":"cli/#available-commands","title":"Available Commands","text":"Command Description <code>models</code> Manage and interact with AI models <code>agents</code> Manage and interact with Oblix agents <code>sessions</code> Manage chat sessions <code>execute</code> Execute a single prompt <code>chat</code> Start an interactive chat session"},{"location":"cli/#authentication","title":"Authentication","text":"<p>Most commands require an Oblix API key, which can be provided in several ways:</p> <ol> <li>Using the <code>--api-key</code> option with each command</li> <li>Setting the <code>OBLIX_API_KEY</code> environment variable</li> <li>When not provided, the CLI will prompt you to enter it</li> </ol> <p>Example: <pre><code>export OBLIX_API_KEY=\"your_oblix_api_key\"\n</code></pre></p>"},{"location":"cli/#common-options","title":"Common Options","text":"<p>These options are available for most commands:</p> Option Description <code>--help</code>, <code>-h</code> Display help for a command <code>--version</code> Show the Oblix CLI version <code>--debug</code> Enable debug logging"},{"location":"cli/#models-commands","title":"Models Commands","text":"<p>Commands for managing AI models:</p>"},{"location":"cli/#list-models","title":"List Models","text":"<p>List all configured models:</p> <pre><code>oblix models list\n</code></pre>"},{"location":"cli/#hook-model","title":"Hook Model","text":"<p>Add a new model to Oblix:</p> <pre><code># Hook an Ollama model\noblix models hook --type ollama --name llama2\n\n# Hook an OpenAI model\noblix models hook --type openai --name gpt-3.5-turbo --api-key-model YOUR_OPENAI_KEY\n\n# Hook a Claude model\noblix models hook --type claude --name claude-3-haiku-20240307 --api-key-model YOUR_ANTHROPIC_KEY\n</code></pre>"},{"location":"cli/#model-info","title":"Model Info","text":"<p>Get information about a specific model:</p> <pre><code>oblix models info --type ollama --name llama2\n</code></pre>"},{"location":"cli/#agents-commands","title":"Agents Commands","text":"<p>Commands for interacting with Oblix agents:</p>"},{"location":"cli/#connectivity","title":"Connectivity","text":"<p>Check current connectivity metrics:</p> <pre><code>oblix agents connectivity\n</code></pre>"},{"location":"cli/#resource","title":"Resource","text":"<p>Perform a system resource check:</p> <pre><code>oblix agents resource\n</code></pre>"},{"location":"cli/#sessions-commands","title":"Sessions Commands","text":"<p>Commands for managing chat sessions:</p>"},{"location":"cli/#list-sessions","title":"List Sessions","text":"<p>List recent chat sessions:</p> <pre><code>oblix sessions list\n</code></pre>"},{"location":"cli/#view-session","title":"View Session","text":"<p>View details of a specific session:</p> <pre><code>oblix sessions view SESSION_ID\n</code></pre>"},{"location":"cli/#delete-session","title":"Delete Session","text":"<p>Delete a specific chat session:</p> <pre><code>oblix sessions delete SESSION_ID\n</code></pre>"},{"location":"cli/#create-session","title":"Create Session","text":"<p>Create a new chat session:</p> <pre><code>oblix sessions create --title \"My New Chat\"\n</code></pre>"},{"location":"cli/#execute-and-chat-commands","title":"Execute and Chat Commands","text":"<p>Execute a single prompt or start an interactive chat:</p>"},{"location":"cli/#execute-a-prompt","title":"Execute a Prompt","text":"<pre><code>oblix execute \"Explain quantum computing in simple terms\"\n</code></pre> <p>With specific model: <pre><code>oblix execute \"Explain quantum computing\" --model \"openai:gpt-3.5-turbo\"\n</code></pre></p>"},{"location":"cli/#start-interactive-chat","title":"Start Interactive Chat","text":"<p>Start an interactive chat session with automatic model orchestration:</p> <pre><code>oblix chat --local-model llama2 --cloud-model gpt-3.5-turbo\n</code></pre> <p>With Claude: <pre><code>oblix chat --local-model llama2 --cloud-model claude-3-haiku --cloud-api-key YOUR_ANTHROPIC_KEY\n</code></pre></p>"},{"location":"cli/#examples","title":"Examples","text":"<p>Here are some common use cases for the Oblix CLI:</p>"},{"location":"cli/#setting-up-hybrid-model-orchestration","title":"Setting Up Hybrid Model Orchestration","text":"<pre><code># Set up local and cloud models for orchestration\noblix models hook --type ollama --name llama2\noblix models hook --type openai --name gpt-3.5-turbo --api-key-model YOUR_OPENAI_KEY\n\n# Start interactive chat with model orchestration\noblix chat --local-model llama2 --cloud-model gpt-3.5-turbo\n</code></pre>"},{"location":"cli/#checking-system-status","title":"Checking System Status","text":"<pre><code># Check system resource metrics\noblix agents resource\n\n# Check connectivity metrics\noblix agents connectivity\n</code></pre>"},{"location":"cli/#managing-sessions","title":"Managing Sessions","text":"<pre><code># List recent sessions\noblix sessions list\n\n# Create a new session\noblix sessions create --title \"Research Project\"\n\n# Execute with a specific session\noblix execute \"Research quantum computing papers\" --session-id YOUR_SESSION_ID\n</code></pre>"},{"location":"cli/#environment-variables","title":"Environment Variables","text":"Variable Description <code>OBLIX_API_KEY</code> Your Oblix API key <code>OPENAI_API_KEY</code> Your OpenAI API key (used as default for OpenAI models)"},{"location":"cli/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues with the CLI:</p> <ol> <li>Ensure you're using the latest version of Oblix</li> <li>Check that you've properly set up your API keys</li> <li>Enable debug logging with the <code>--debug</code> flag</li> <li>Verify that the Ollama server is running if using local models</li> <li>Confirm your Mac meets the system requirements (particularly for larger models)</li> </ol> <p>For additional help, join the Oblix Discord community at https://discord.gg/v8qtEVuU.</p>"},{"location":"core-concepts/","title":"Core Concepts","text":"<p>This section explains the fundamental concepts behind the Oblix SDK and how it works to provide intelligent AI model orchestration.</p>"},{"location":"core-concepts/#what-is-model-orchestration","title":"What is Model Orchestration?","text":"<p>Model orchestration is the process of managing and coordinating multiple AI models to optimize for factors such as:</p> <ul> <li>Performance and latency</li> <li>Resource utilization</li> <li>Cost efficiency</li> <li>Reliability</li> <li>Connectivity status</li> </ul> <p>Rather than manually deciding which model to use in different situations, Oblix automates this decision-making process based on real-time system metrics and connectivity data.</p>"},{"location":"core-concepts/#how-oblix-works","title":"How Oblix Works","text":"<p>At its core, Oblix works by:</p> <ol> <li>Providing a unified interface for multiple model providers (OpenAI, Claude, Ollama)</li> <li>Monitoring system metrics through specialized agents</li> <li>Evaluating routing policies based on collected metrics</li> <li>Intelligently routing executions to the optimal model</li> <li>Managing session context for conversational interactions</li> </ol>"},{"location":"core-concepts/#key-components","title":"Key Components","text":""},{"location":"core-concepts/#oblixclient","title":"OblixClient","text":"<p>The main entry point for the SDK provides methods for:</p> <ul> <li>Hooking models from different providers</li> <li>Adding monitoring agents</li> <li>Executing prompts</li> <li>Managing chat sessions</li> </ul>"},{"location":"core-concepts/#models","title":"Models","text":"<p>Oblix supports multiple model types:</p> <ul> <li>OLLAMA: Local models running via Ollama</li> <li>OPENAI: Cloud models from OpenAI (GPT series)</li> <li>CLAUDE: Cloud models from Anthropic (Claude series)</li> <li>CUSTOM: Custom model implementations</li> </ul>"},{"location":"core-concepts/#agents","title":"Agents","text":"<p>Specialized components that monitor system state:</p> <ul> <li>ResourceMonitor: Tracks CPU, memory, and GPU usage</li> <li>ConnectivityAgent: Monitors network connectivity quality</li> </ul>"},{"location":"core-concepts/#execution-flow","title":"Execution Flow","text":"<p>When you execute a prompt with Oblix:</p> <ol> <li>All registered agents perform checks on system resources and connectivity</li> <li>Routing policies evaluate the collected metrics</li> <li>The optimal model is selected based on the current state</li> <li>The prompt is sent to the selected model</li> <li>The response is returned along with execution metadata</li> </ol>"},{"location":"core-concepts/#hybrid-execution-strategy","title":"Hybrid Execution Strategy","text":"<p>Oblix implements a hybrid execution strategy that intelligently chooses between:</p> <ul> <li>Local execution for better privacy, lower latency, and offline capability</li> <li>Cloud execution for more powerful models and when local resources are constrained</li> </ul>"},{"location":"core-concepts/#benefits","title":"Benefits","text":"<p>This orchestration approach provides several key benefits:</p> <ul> <li>Resilience: Continue operating even when connectivity is limited or lost</li> <li>Optimization: Use the most cost-effective model for each task</li> <li>Performance: Select models based on current system capabilities</li> <li>Simplicity: Provide a single interface for all model interactions</li> <li>Adaptability: Automatically adjust to changing conditions</li> </ul> <p>In the following sections, we'll dive deeper into each of these core concepts:</p> <ul> <li>Orchestration: How the routing system works</li> <li>Agents: Monitoring system state</li> <li>Session Management: Handling conversations</li> </ul> <p>Understanding these concepts will help you get the most out of the Oblix SDK in your applications.</p>"},{"location":"core-concepts/agents/","title":"Agents","text":"<p>Agents are a core component of the Oblix orchestration system that provide awareness of system state and influence model orchestration decisions. This page explains how agents work and how they are used to optimize AI model execution.</p>"},{"location":"core-concepts/agents/#what-are-agents","title":"What Are Agents?","text":"<p>In Oblix, agents are specialized components that:</p> <ol> <li>Monitor specific aspects of the system (resources, connectivity, etc.)</li> <li>Evaluate the current state against defined thresholds</li> <li>Recommend execution targets based on their findings</li> <li>Provide detailed metrics for transparency and debugging</li> </ol> <p>Agents run continuously in the background, ensuring the Oblix system always has up-to-date information about the execution environment to make intelligent orchestration decisions.</p>"},{"location":"core-concepts/agents/#built-in-agent-types","title":"Built-in Agent Types","text":""},{"location":"core-concepts/agents/#resourcemonitor","title":"ResourceMonitor","text":"<p>The <code>ResourceMonitor</code> agent tracks system resource utilization, including:</p> <ul> <li>CPU usage - Overall processor utilization percentage</li> <li>Memory usage - RAM utilization percentage</li> <li>System load - Average system load over time</li> <li>GPU availability - Presence and utilization of GPU resources</li> </ul> <p>Based on these metrics, the ResourceMonitor recommends one of several execution targets:</p> <ul> <li><code>LOCAL_CPU</code> - Execute on local CPU (when resources are available)</li> <li><code>LOCAL_GPU</code> - Execute on local GPU (when available and suitable)</li> <li><code>CLOUD</code> - Execute on cloud models (when local resources are constrained)</li> </ul> <pre><code>from oblix.agents.resource_monitor import ResourceMonitor\n\n# Create with default settings\nresource_monitor = ResourceMonitor()\n\n# Add to client\nclient.hook_agent(resource_monitor)\n</code></pre>"},{"location":"core-concepts/agents/#resource-states","title":"Resource States","text":"<p>The ResourceMonitor reports one of three states:</p> <ul> <li><code>AVAILABLE</code> - Sufficient resources available for local execution</li> <li><code>CONSTRAINED</code> - Resources are limited but usable</li> <li><code>CRITICAL</code> - Resources are extremely limited, recommend cloud execution</li> </ul>"},{"location":"core-concepts/agents/#connectivityagent","title":"ConnectivityAgent","text":"<p>The <code>ConnectivityAgent</code> monitors network connectivity, including:</p> <ul> <li>Connection type - Type of network connection (wifi, ethernet, etc.)</li> <li>Latency - Response time to key endpoints</li> <li>Packet loss - Percentage of lost packets</li> <li>Bandwidth - Available network bandwidth</li> </ul> <p>Based on these metrics, the ConnectivityAgent recommends one of several orchestration targets:</p> <ul> <li><code>LOCAL</code> - Use local models due to connectivity issues</li> <li><code>CLOUD</code> - Use cloud models due to good connectivity</li> <li><code>HYBRID</code> - Consider balanced execution between local and cloud</li> </ul> <pre><code>from oblix.agents.connectivity import ConnectivityAgent\n\n# Create with default settings\nconnectivity_agent = ConnectivityAgent()\n\n# Add to client\nclient.hook_agent(connectivity_agent)\n</code></pre>"},{"location":"core-concepts/agents/#connectivity-states","title":"Connectivity States","text":"<p>The ConnectivityAgent reports one of three states:</p> <ul> <li><code>OPTIMAL</code> - Good connectivity suitable for cloud model use</li> <li><code>DEGRADED</code> - Limited connectivity that may affect performance</li> <li><code>DISCONNECTED</code> - No connectivity, must use local models</li> </ul>"},{"location":"core-concepts/agents/#agent-lifecycle","title":"Agent Lifecycle","text":"<p>Agents follow a defined lifecycle:</p> <ol> <li>Initialization - Setting up monitoring capabilities</li> <li>Periodic Checks - Collecting metrics at regular intervals</li> <li>On-demand Checks - Performing checks when explicitly requested</li> <li>Shutdown - Gracefully releasing resources</li> </ol>"},{"location":"core-concepts/agents/#agent-orchestration-process","title":"Agent Orchestration Process","text":"<p>When you execute a prompt, all registered agents perform checks to inform the orchestration decision:</p> <pre><code>response = await client.execute(\"Explain quantum computing\")\n\n# Orchestration decisions are included in the response\nagent_checks = response[\"agent_checks\"]\n</code></pre> <p>The <code>agent_checks</code> dictionary contains detailed information about:</p> <ul> <li>Current system state assessments</li> <li>Recommended execution targets</li> <li>Specific metrics collected</li> <li>Reasoning for orchestration decisions</li> </ul>"},{"location":"core-concepts/agents/#customizing-agent-behavior","title":"Customizing Agent Behavior","text":"<p>You can customize agent behavior by providing configuration options:</p>"},{"location":"core-concepts/agents/#resource-thresholds","title":"Resource Thresholds","text":"<pre><code># Create with custom thresholds\nresource_monitor = ResourceMonitor(\n    custom_thresholds={\n        \"cpu_threshold\": 70.0,     # CPU usage percentage (default: 80.0)\n        \"memory_threshold\": 75.0,  # RAM usage percentage (default: 85.0)\n        \"load_threshold\": 3.0,     # System load average (default: 4.0)\n        \"gpu_threshold\": 75.0      # GPU utilization percentage (default: 85.0)\n    }\n)\n</code></pre>"},{"location":"core-concepts/agents/#connectivity-thresholds","title":"Connectivity Thresholds","text":"<pre><code># Create with custom connectivity thresholds\nconnectivity_agent = ConnectivityAgent(\n    latency_threshold=150.0,       # Maximum acceptable latency in ms (default: 200.0)\n    packet_loss_threshold=5.0,     # Maximum acceptable packet loss percentage (default: 10.0)\n    bandwidth_threshold=10.0,      # Minimum acceptable bandwidth in Mbps (default: 5.0)\n    check_interval=60              # Seconds between connectivity checks (default: 30)\n)\n</code></pre>"},{"location":"core-concepts/agents/#intelligent-orchestration-system","title":"Intelligent Orchestration System","text":"<p>Oblix's agent architecture powers its intelligent orchestration system with several key benefits:</p> <ol> <li>Adaptive Execution - Dynamically selects the optimal model based on real-time conditions</li> <li>Resilience - Automatically falls back to local models when connectivity fails</li> <li>Resource Optimization - Balances workload between local and cloud resources</li> <li>Performance Tuning - Configurable thresholds to match specific hardware capabilities</li> <li>Transparency - Clear visibility into orchestration decisions</li> </ol>"},{"location":"core-concepts/agents/#orchestration-decision-flow","title":"Orchestration Decision Flow","text":"<p>The Oblix orchestration system follows this decision process:</p> <ol> <li>Resource Assessment - ResourceMonitor evaluates system capabilities</li> <li>Connectivity Verification - ConnectivityAgent checks network conditions</li> <li>Decision Prioritization - Weighs connectivity status higher than resource availability</li> <li>Model Selection - Chooses appropriate model based on agent recommendations</li> <li>Execution - Runs the prompt on the selected model</li> </ol> <p>This orchestration flow ensures that your AI workloads run on the most appropriate model given the current system conditions.</p>"},{"location":"core-concepts/agents/#best-practices","title":"Best Practices","text":"<p>When working with Oblix's agent-based orchestration:</p> <ul> <li>Enable Both Agents - Use both ResourceMonitor and ConnectivityAgent together for optimal orchestration</li> <li>Start with Defaults - Use the default agent thresholds initially</li> <li>Monitor Performance - Review agent recommendations over time</li> <li>Adjust Gradually - Make small adjustments to thresholds based on observations</li> <li>Configure for Your Environment - Tune thresholds based on your specific hardware capabilities</li> </ul> <p>By leveraging Oblix's agent-based orchestration system effectively, you can build AI applications that intelligently adapt to changing conditions while optimizing for performance, cost, and reliability.</p>"},{"location":"core-concepts/how-it-works/","title":"How Oblix Works","text":"<p>Oblix is an AI model orchestration SDK that intelligently routes between local and cloud-based models based on connectivity, system resources, and other factors. This page explains the core architecture and concepts that power Oblix.</p>"},{"location":"core-concepts/how-it-works/#core-architecture","title":"Core Architecture","text":"<p>Oblix consists of several key components:</p> <ol> <li>Client Interface - The main entry point (<code>OblixClient</code>) that developers interact with</li> <li>Model Management - Handles various model types from different providers</li> <li>Agent System - Monitors system state and makes routing decisions</li> <li>Session Management - Maintains persistent chat sessions</li> <li>Execution Engine - Orchestrates the execution flow between models</li> </ol>"},{"location":"core-concepts/how-it-works/#intelligent-routing","title":"Intelligent Routing","text":"<p>Oblix's intelligent routing system works by:</p> <ol> <li>Monitoring Resources - Tracking CPU, memory, and GPU utilization</li> <li>Checking Connectivity - Monitoring network quality and availability  </li> <li>Applying Policies - Using configurable policies to make routing decisions</li> <li>Executing Actions - Dynamically switching between models based on current conditions</li> </ol>"},{"location":"core-concepts/how-it-works/#example-routing-scenarios","title":"Example Routing Scenarios","text":"Scenario Resource State Connectivity Routing Decision Offline work Available Disconnected Route to local Ollama model Limited bandwidth Available Degraded Use smaller cloud model or local model Resource constrained Constrained Optimal Route to cloud model Optimal conditions Available Optimal Prefer local model for speed"},{"location":"core-concepts/how-it-works/#agent-system","title":"Agent System","text":"<p>Agents are pluggable components that monitor specific aspects of the system:</p>"},{"location":"core-concepts/how-it-works/#resourcemonitor","title":"ResourceMonitor","text":"<p>Tracks system resources including: - CPU utilization and load - Memory availability - GPU availability and utilization</p> <p>Based on resource metrics, it recommends the optimal execution target (local CPU, local GPU, or cloud).</p>"},{"location":"core-concepts/how-it-works/#connectivityagent","title":"ConnectivityAgent","text":"<p>Monitors network connectivity including: - Connection type (wifi, ethernet, etc.) - Latency and packet loss - Available bandwidth - Connection stability</p> <p>The connectivity agent adapts to changing network conditions, ensuring reliability even in challenging environments.</p>"},{"location":"core-concepts/how-it-works/#model-abstraction","title":"Model Abstraction","text":"<p>Oblix provides a unified interface for working with different model types:</p> <ul> <li>Ollama Models - Local models running via Ollama</li> <li>OpenAI Models - Cloud models from OpenAI (GPT series)</li> <li>Claude Models - Cloud models from Anthropic (Claude series)</li> </ul> <p>Each model implementation handles: - Authentication and API communication - Token counting and management - Performance metrics collection - Error handling and retries</p>"},{"location":"core-concepts/how-it-works/#session-management","title":"Session Management","text":"<p>Oblix includes a built-in session management system that:</p> <ul> <li>Maintains conversation history</li> <li>Provides persistence across application restarts</li> <li>Enables multi-session workflows</li> <li>Handles context management for stateful conversations</li> </ul>"},{"location":"core-concepts/how-it-works/#execution-flow","title":"Execution Flow","text":"<p>When you execute a prompt with Oblix, here's what happens behind the scenes:</p> <ol> <li>Pre-execution Checks - All registered agents perform checks and make recommendations</li> <li>Model Selection - The system selects the optimal model based on agent recommendations</li> <li>Context Retrieval - If in a session, relevant context is loaded</li> <li>Execution - The prompt is sent to the selected model for processing</li> <li>Metrics Collection - Performance metrics are collected during execution</li> <li>Response Handling - The response is returned along with metadata</li> <li>Session Update - If in a session, the interaction is saved</li> </ol>"},{"location":"core-concepts/how-it-works/#configuration-system","title":"Configuration System","text":"<p>Oblix stores configuration persistently, including:</p> <ul> <li>Model configurations</li> <li>API keys (securely)</li> <li>Custom thresholds</li> <li>User preferences</li> </ul> <p>The configuration system provides portability and simplifies repeated use.</p>"},{"location":"core-concepts/how-it-works/#optimizing-for-developer-experience","title":"Optimizing for Developer Experience","text":"<p>Oblix abstracts away the complexity of working with multiple models, allowing developers to:</p> <ul> <li>Write code once that works across changing environments</li> <li>Gracefully handle offline scenarios</li> <li>Optimize for performance and cost</li> <li>Maintain conversation context seamlessly</li> </ul> <p>This design ensures that AI applications built with Oblix are more resilient, adaptive, and user-friendly than traditional approaches.</p>"},{"location":"core-concepts/orchestration/","title":"Orchestration","text":"<p>Here's the complete content for your <code>core-concepts/orchestration.md</code> file:</p>"},{"location":"core-concepts/orchestration/#orchestration","title":"Orchestration","text":"<p>Orchestration is the heart of the Oblix SDK, providing intelligent routing between local and cloud models based on system state. This page explains how Oblix's orchestration system works.</p>"},{"location":"core-concepts/orchestration/#the-orchestration-challenge","title":"The Orchestration Challenge","text":"<p>AI applications face several challenges when deciding which models to use:</p> <ul> <li>Resource constraints: Local devices may have limited CPU, memory, or GPU resources</li> <li>Connectivity issues: Network quality can vary or be completely unavailable</li> <li>Cost considerations: Cloud models incur API costs per token</li> <li>Performance requirements: Response time may be critical for certain applications</li> <li>Capability needs: Some tasks require more advanced models</li> </ul> <p>Traditionally, developers had to manually implement logic to handle these scenarios, leading to complex conditional code or simplified approaches that don't adapt to changing conditions.</p>"},{"location":"core-concepts/orchestration/#oblixs-orchestration-solution","title":"Oblix's Orchestration Solution","text":"<p>Oblix solves this challenge with a dynamic orchestration system that:</p> <ol> <li>Continuously monitors system resources and connectivity</li> <li>Applies configurable policies to determine the optimal execution target</li> <li>Automatically routes prompts to the best available model</li> <li>Provides transparency into routing decisions</li> </ol>"},{"location":"core-concepts/orchestration/#orchestration-components","title":"Orchestration Components","text":""},{"location":"core-concepts/orchestration/#monitoring-agents","title":"Monitoring Agents","text":"<p>Agents collect real-time metrics about the system state:</p> <ul> <li>ResourceMonitor: Collects CPU, memory, and GPU utilization metrics</li> <li>ConnectivityAgent: Measures latency, packet loss, and bandwidth</li> </ul>"},{"location":"core-concepts/orchestration/#policy-evaluation","title":"Policy Evaluation","text":"<p>Policies define the rules for routing decisions:</p> <ul> <li>ResourcePolicy: Evaluates system resource metrics against thresholds</li> <li>ConnectivityPolicy: Evaluates network quality against thresholds</li> </ul>"},{"location":"core-concepts/orchestration/#routing-decision","title":"Routing Decision","text":"<p>Based on policy evaluation, the orchestration system determines:</p> <ul> <li>Which model provider to use (Ollama, OpenAI, Claude)</li> <li>Which specific model to execute the prompt with</li> <li>What parameters to apply for optimal performance</li> </ul>"},{"location":"core-concepts/orchestration/#orchestration-flow","title":"Orchestration Flow","text":"<p>When you execute a prompt, the orchestration process follows these steps:</p> <ol> <li>Agent Checks: All registered agents perform checks and report their findings</li> <li>Policy Evaluation: Resource and connectivity policies evaluate the current state</li> <li>Target Selection: The system selects the optimal execution target</li> <li>Model Selection: The appropriate model is chosen from the target provider</li> <li>Execution: The prompt is sent to the selected model</li> <li>Response Handling: The response is returned along with execution metadata</li> </ol>"},{"location":"core-concepts/orchestration/#orchestration-states","title":"Orchestration States","text":"<p>The orchestration system can be in various states that influence routing decisions:</p>"},{"location":"core-concepts/orchestration/#resource-states","title":"Resource States","text":"State Description Typical Routing AVAILABLE Sufficient resources available Local execution preferred CONSTRAINED Limited resources available Smaller local models or cloud CRITICAL Extremely limited resources Cloud execution required"},{"location":"core-concepts/orchestration/#connectivity-states","title":"Connectivity States","text":"State Description Typical Routing OPTIMAL Good connectivity Cloud execution viable DEGRADED Limited connectivity Smaller cloud models or local DISCONNECTED No connectivity Local execution required"},{"location":"core-concepts/orchestration/#configuring-orchestration","title":"Configuring Orchestration","text":"<p>You can customize the orchestration system by:</p>"},{"location":"core-concepts/orchestration/#custom-resource-thresholds","title":"Custom Resource Thresholds","text":"<pre><code>from oblix.agents import ResourceMonitor\n\n# Create with custom thresholds\nresource_monitor = ResourceMonitor(\n    custom_thresholds={\n        \"cpu_threshold\": 70.0,     # CPU usage percentage (default: 80.0)\n        \"memory_threshold\": 75.0,  # RAM usage percentage (default: 85.0)\n        \"load_threshold\": 3.0,     # System load average (default: 4.0)\n        \"gpu_threshold\": 75.0      # GPU utilization percentage (default: 85.0)\n    }\n)\n</code></pre>"},{"location":"core-concepts/orchestration/#custom-connectivity-thresholds","title":"Custom Connectivity Thresholds","text":"<pre><code>from oblix.agents import ConnectivityAgent\n\n# Create with custom connectivity thresholds\nconnectivity_agent = ConnectivityAgent(\n    latency_threshold=150.0,       # Maximum acceptable latency in ms (default: 200.0)\n    packet_loss_threshold=5.0,     # Maximum acceptable packet loss percentage (default: 10.0)\n    bandwidth_threshold=10.0       # Minimum acceptable bandwidth in Mbps (default: 5.0)\n)\n</code></pre>"},{"location":"core-concepts/orchestration/#transparency-and-debugging","title":"Transparency and Debugging","text":"<p>Oblix provides transparency into orchestration decisions through the response metadata:</p> <pre><code>response = await client.execute(\"Explain quantum computing\")\n\n# Access orchestration decision data\nagent_checks = response[\"agent_checks\"]\nprint(f\"Resource state: {agent_checks.get('resource_monitor', {}).get('state')}\")\nprint(f\"Connectivity: {agent_checks.get('connectivity_monitor', {}).get('state')}\")\nprint(f\"Selected model: {response['model_id']}\")\n</code></pre> <p>This transparency helps you understand why specific routing decisions were made and can be valuable for debugging or fine-tuning the orchestration system.</p>"},{"location":"core-concepts/orchestration/#fallback-mechanisms","title":"Fallback Mechanisms","text":"<p>If a preferred model is unavailable or fails, Oblix includes fallback mechanisms:</p> <ol> <li>Primary execution attempt with the optimal model</li> <li>Fallback to alternative models if the primary fails</li> <li>Degraded mode operation when connectivity is limited</li> </ol> <p>This ensures robustness even in challenging environments.</p>"},{"location":"core-concepts/orchestration/#advanced-orchestration-features","title":"Advanced Orchestration Features","text":""},{"location":"core-concepts/orchestration/#model-specific-routing","title":"Model-Specific Routing","text":"<p>You can override automatic routing for specific prompts:</p> <pre><code># Force execution with a specific model\nresponse = await client.execute(\n    \"Explain quantum computing\",\n    model_id=\"openai:gpt-4\"\n)\n</code></pre>"},{"location":"core-concepts/orchestration/#session-based-routing","title":"Session-Based Routing","text":"<p>For chat sessions, Oblix maintains routing consistency when possible, to provide a seamless experience:</p> <pre><code># Start a chat session (routing decisions persist across messages)\nawait client.start_chat()\n</code></pre>"},{"location":"core-concepts/orchestration/#multi-model-execution","title":"Multi-Model Execution","text":"<p>For critical applications, you can implement multi-model execution strategies:</p> <pre><code># Execute with multiple models and compare results\nresponse1 = await client.execute(\"Explain quantum computing\", model_id=\"openai:gpt-4\")\nresponse2 = await client.execute(\"Explain quantum computing\", model_id=\"claude:claude-3-opus-20240229\")\n</code></pre> <p>By understanding and configuring Oblix's orchestration system, you can build AI applications that seamlessly adapt to changing conditions while optimizing for cost, performance, and reliability.</p>"},{"location":"core-concepts/session-management/","title":"Session Management","text":"<p>Oblix provides built-in session management capabilities to handle conversational interactions with AI models. This page explains how sessions work and how to use them effectively.</p>"},{"location":"core-concepts/session-management/#what-are-sessions","title":"What are Sessions?","text":"<p>Sessions in Oblix are persistent containers for conversation history that:</p> <ol> <li>Store messages between users and AI models</li> <li>Maintain context across multiple interactions</li> <li>Persist across application restarts</li> <li>Enable stateful conversations</li> </ol> <p>Sessions are particularly useful for building chat applications, virtual assistants, or any interactive AI experience where context matters.</p>"},{"location":"core-concepts/session-management/#session-lifecycle","title":"Session Lifecycle","text":"<p>A typical session lifecycle includes:</p> <ol> <li>Creation - A new session is created with a unique ID</li> <li>Message Exchange - User and AI messages are added to the session</li> <li>Context Maintenance - Context is preserved between interactions</li> <li>Persistence - Session data is saved to disk</li> <li>Retrieval - Sessions can be loaded by ID for continued interaction</li> <li>Deletion - Sessions can be deleted when no longer needed</li> </ol>"},{"location":"core-concepts/session-management/#working-with-sessions","title":"Working with Sessions","text":""},{"location":"core-concepts/session-management/#creating-a-session","title":"Creating a Session","text":"<pre><code>from oblix import OblixClient\n\nclient = OblixClient(oblix_api_key=\"your_api_key\")\n\n# Create a new session\nsession_id = await client.create_session(\n    title=\"Customer Support Chat\",\n    initial_context={\"customer_id\": \"cust_123\", \"subscription_tier\": \"premium\"}\n)\n\nprint(f\"Created session: {session_id}\")\n</code></pre>"},{"location":"core-concepts/session-management/#executing-prompts-in-a-session","title":"Executing Prompts in a Session","text":"<pre><code># Set the current session\nclient.current_session_id = session_id\n\n# Execute prompts in the session context\nresponse = await client.execute(\"Hello, I'm having trouble with my account\")\n\n# The context is automatically maintained\nfollow_up_response = await client.execute(\"Can you help me reset my password?\")\n</code></pre>"},{"location":"core-concepts/session-management/#using-chat-methods","title":"Using Chat Methods","text":"<p>For chat applications, Oblix provides simplified methods:</p> <pre><code># Send a single message in a session\nresponse = await client.chat_once(\"Hello, how can I help you today?\")\nsession_id = response[\"session_id\"]\n\n# Continue the conversation in the same session\nresponse = await client.chat_once(\"I need help with my subscription\", session_id)\n</code></pre>"},{"location":"core-concepts/session-management/#interactive-chat-sessions","title":"Interactive Chat Sessions","text":"<p>For command-line or interactive applications:</p> <pre><code># Start an interactive chat session\nawait client.start_chat()\n\n# Or resume an existing session\nawait client.start_chat(session_id=\"existing_session_id\")\n</code></pre>"},{"location":"core-concepts/session-management/#managing-sessions","title":"Managing Sessions","text":"<p>Oblix provides methods to list, load, and delete sessions:</p> <pre><code># List recent sessions\nsessions = client.list_sessions(limit=10)\nfor session in sessions:\n    print(f\"ID: {session['id']} | Title: {session['title']} | Messages: {session['message_count']}\")\n\n# Load a specific session\nsession_data = client.load_session(\"session_id_here\")\n\n# Delete a session\nsuccess = client.delete_session(\"session_id_here\")\n</code></pre>"},{"location":"core-concepts/session-management/#session-storage","title":"Session Storage","text":"<p>By default, Oblix stores sessions as JSON files in a local directory:</p> <ul> <li>Default location: <code>~/.oblix/sessions/</code></li> <li>Format: Each session is stored as a separate JSON file</li> <li>Naming: Files are named using the session ID</li> </ul> <p>You can customize the storage location:</p> <pre><code>from oblix.sessions import SessionManager\n\n# Custom session directory\nsession_manager = SessionManager(base_dir=\"/path/to/custom/directory\")\n\n# Use with client\nclient = OblixClient(oblix_api_key=\"your_api_key\")\nclient.session_manager = session_manager\n</code></pre>"},{"location":"core-concepts/session-management/#session-structure","title":"Session Structure","text":"<p>Each session contains:</p> <ul> <li>ID: Unique identifier for the session</li> <li>Title: Human-readable title for the session</li> <li>Created/Updated: Timestamps for creation and last update</li> <li>Messages: Array of message objects with:</li> <li>Role: 'user' or 'assistant'</li> <li>Content: Message text</li> <li>Timestamp: When the message was added</li> <li>Context: Optional dictionary for additional context</li> </ul> <p>Example session structure:</p> <pre><code>{\n  \"id\": \"5f3e9a7b-6c1d-4d8e-9e7a-9b8c7d6e5f4a\",\n  \"title\": \"Technical Support Chat\",\n  \"created_at\": \"2023-07-15T14:30:22.123Z\",\n  \"updated_at\": \"2023-07-15T14:35:46.789Z\",\n  \"messages\": [\n    {\n      \"id\": \"msg_1\",\n      \"role\": \"user\",\n      \"content\": \"I'm having trouble installing the software.\",\n      \"timestamp\": \"2023-07-15T14:30:22.123Z\"\n    },\n    {\n      \"id\": \"msg_2\",\n      \"role\": \"assistant\",\n      \"content\": \"I'm sorry to hear that. What operating system are you using?\",\n      \"timestamp\": \"2023-07-15T14:30:25.456Z\"\n    },\n    {\n      \"id\": \"msg_3\",\n      \"role\": \"user\",\n      \"content\": \"I'm using Windows 11.\",\n      \"timestamp\": \"2023-07-15T14:35:46.789Z\"\n    }\n  ],\n  \"context\": {\n    \"user_id\": \"user_789\",\n    \"product\": \"Oblix SDK\"\n  }\n}\n</code></pre>"},{"location":"core-concepts/session-management/#context-management","title":"Context Management","text":"<p>Sessions maintain conversation history, which is used as context for future interactions. By default, Oblix includes recent messages when generating responses, allowing models to understand the conversation flow.</p> <p>You can also provide additional context when creating a session:</p> <pre><code># Create session with initial context\nsession_id = await client.create_session(\n    initial_context={\n        \"user_name\": \"Alex\",\n        \"preferences\": {\n            \"language\": \"English\",\n            \"format\": \"concise\"\n        }\n    }\n)\n</code></pre>"},{"location":"core-concepts/session-management/#model-consistency-in-sessions","title":"Model Consistency in Sessions","text":"<p>When using sessions with multiple available models, Oblix attempts to maintain consistency by:</p> <ol> <li>Using the same model for all messages in a session when possible</li> <li>Falling back to alternative models only when necessary</li> <li>Including sufficient context to maintain conversation flow even when switching models</li> </ol>"},{"location":"core-concepts/session-management/#advanced-session-features","title":"Advanced Session Features","text":""},{"location":"core-concepts/session-management/#session-metadata","title":"Session Metadata","text":"<p>You can access and update session metadata:</p> <pre><code># Get session data\nsession_data = client.load_session(session_id)\n\n# Access metadata\ntitle = session_data[\"title\"]\ncreated_at = session_data[\"created_at\"]\nmessage_count = len(session_data[\"messages\"])\n</code></pre>"},{"location":"core-concepts/session-management/#message-filtering","title":"Message Filtering","text":"<p>When working with long conversations, you can filter messages:</p> <pre><code># Get only the most recent messages\nrecent_messages = session_data[\"messages\"][-5:]\n\n# Filter by role\nuser_messages = [msg for msg in session_data[\"messages\"] if msg[\"role\"] == \"user\"]\n</code></pre>"},{"location":"core-concepts/session-management/#context-window-management","title":"Context Window Management","text":"<p>For long conversations that might exceed model context windows, Oblix automatically:</p> <ol> <li>Selects the most relevant messages as context</li> <li>Prioritizes recent messages</li> <li>Includes critical context from earlier in the conversation</li> </ol>"},{"location":"core-concepts/session-management/#best-practices","title":"Best Practices","text":"<p>When working with sessions:</p> <ul> <li>Create dedicated sessions for different conversation types</li> <li>Include relevant initial context to guide the conversation</li> <li>Clean up old sessions to manage storage</li> <li>Monitor session length for very long conversations</li> <li>Consider model consistency when choosing models for sessions</li> </ul> <p>By effectively using Oblix's session management capabilities, you can build conversational AI applications that maintain context and provide a natural, coherent user experience.</p>"},{"location":"examples/","title":"Examples","text":"<p>This section provides practical examples to help you understand how to use Oblix in different scenarios. These examples demonstrate common patterns and best practices for integrating Oblix into your applications.</p>"},{"location":"examples/#basic-examples","title":"Basic Examples","text":"<p>Start with these examples to learn the fundamentals of Oblix:</p> <ul> <li>Basic Usage: Simple examples to get started with Oblix</li> <li>Hybrid Execution: Examples of combining local and cloud models</li> </ul>"},{"location":"examples/#use-cases","title":"Use Cases","text":"<p>These examples showcase how to use Oblix for specific use cases:</p>"},{"location":"examples/#ai-assistants","title":"AI Assistants","text":"<ul> <li>Building conversational agents that maintain context</li> <li>Creating domain-specific assistants with knowledge bases</li> <li>Implementing multi-turn conversations with stateful sessions</li> </ul>"},{"location":"examples/#content-generation","title":"Content Generation","text":"<ul> <li>Text generation across multiple models</li> <li>Document summarization with fallback mechanisms</li> <li>Creative writing with model-specific strengths</li> </ul>"},{"location":"examples/#development-workflows","title":"Development Workflows","text":"<ul> <li>Using Oblix for code generation and explanation</li> <li>Building development tools with AI assistance</li> <li>Creating intelligent testing and debugging helpers</li> </ul>"},{"location":"examples/#environment-specific-examples","title":"Environment-Specific Examples","text":"<p>Examples tailored to different environments:</p>"},{"location":"examples/#desktop-applications","title":"Desktop Applications","text":"<ul> <li>Integrating Oblix into desktop apps for offline capability</li> <li>Optimizing for CPU/GPU switching on personal computers</li> <li>Managing resource usage for background AI processing</li> </ul>"},{"location":"examples/#mobile-and-edge-devices","title":"Mobile and Edge Devices","text":"<ul> <li>Adapting to limited connectivity scenarios</li> <li>Optimizing for battery and resource constraints</li> <li>Implementing progressive enhancement based on connectivity</li> </ul>"},{"location":"examples/#server-environments","title":"Server Environments","text":"<ul> <li>Scaling Oblix in high-throughput environments</li> <li>Load balancing between local and cloud models</li> <li>Implementing redundancy and fallback strategies</li> </ul>"},{"location":"examples/#code-examples-repository","title":"Code Examples Repository","text":"<p>For more comprehensive examples and sample applications, visit our GitHub examples repository.</p>"},{"location":"examples/#contributing","title":"Contributing","text":"<p>Have you built something interesting with Oblix? We'd love to feature your example! Submit a pull request to our examples repository or reach out to us at examples@oblixai.com.</p>"},{"location":"examples/basic-usage/","title":"Basic Usage","text":"<p>This page provides basic examples to help you get started with Oblix. These examples cover installation, initialization, model hooking, and executing prompts with the SDK.</p>"},{"location":"examples/basic-usage/#installation","title":"Installation","text":"<p>Oblix is available exclusively for macOS at this time:</p> <ol> <li>Visit oblix.ai to download the latest version</li> <li>Follow the installation instructions on the website</li> <li>Once installed, you can import the SDK in your Python projects</li> </ol> <p>Note: Currently, only macOS is supported. Windows and Linux versions are planned for future releases.</p>"},{"location":"examples/basic-usage/#initialization","title":"Initialization","text":"<p>Initialize the Oblix client with your API key:</p> <pre><code>import asyncio\nfrom oblix import OblixClient\n\nasync def main():\n    # Initialize with API key\n    client = OblixClient(oblix_api_key=\"your_oblix_api_key\")\n\n    # Alternatively, set the OBLIX_API_KEY environment variable\n    # and initialize without explicitly providing the key\n    # client = OblixClient()\n\n    # Rest of your code here...\n\n# Run the async function\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/basic-usage/#hooking-models","title":"Hooking Models","text":"<p>Add models to your client for execution:</p> <pre><code>import asyncio\nfrom oblix import OblixClient, ModelType\n\nasync def main():\n    client = OblixClient(oblix_api_key=\"your_oblix_api_key\")\n\n    # Hook a local Ollama model\n    await client.hook_model(\n        model_type=ModelType.OLLAMA,\n        model_name=\"llama2\"\n    )\n\n    # Hook an OpenAI model\n    await client.hook_model(\n        model_type=ModelType.OPENAI,\n        model_name=\"gpt-3.5-turbo\",\n        api_key=\"your_openai_api_key\"\n    )\n\n    # Hook a Claude model\n    await client.hook_model(\n        model_type=ModelType.CLAUDE,\n        model_name=\"claude-3-opus-20240229\",\n        api_key=\"your_anthropic_api_key\"\n    )\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/basic-usage/#adding-monitoring-agents","title":"Adding Monitoring Agents","text":"<p>Add monitoring agents to enable intelligent orchestration:</p> <pre><code>import asyncio\nfrom oblix import OblixClient, ModelType\nfrom oblix.agents.resource_monitor import ResourceMonitor\nfrom oblix.agents.connectivity import ConnectivityAgent\n\nasync def main():\n    client = OblixClient(oblix_api_key=\"your_oblix_api_key\")\n\n    # Hook models\n    await client.hook_model(ModelType.OLLAMA, \"llama2\")\n    await client.hook_model(\n        ModelType.OPENAI, \n        \"gpt-3.5-turbo\", \n        api_key=\"your_openai_api_key\"\n    )\n\n    # Add resource monitoring\n    client.hook_agent(ResourceMonitor())\n\n    # Add connectivity monitoring\n    client.hook_agent(ConnectivityAgent())\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/basic-usage/#executing-prompts","title":"Executing Prompts","text":"<p>Execute prompts with automatic orchestration:</p> <pre><code>import asyncio\nfrom oblix import OblixClient, ModelType\nfrom oblix.agents.resource_monitor import ResourceMonitor\nfrom oblix.agents.connectivity import ConnectivityAgent\n\nasync def main():\n    client = OblixClient(oblix_api_key=\"your_oblix_api_key\")\n\n    # Hook models\n    await client.hook_model(ModelType.OLLAMA, \"llama2\")\n    await client.hook_model(\n        ModelType.OPENAI, \n        \"gpt-3.5-turbo\", \n        api_key=\"your_openai_api_key\"\n    )\n\n    # Add monitoring agents\n    client.hook_agent(ResourceMonitor())\n    client.hook_agent(ConnectivityAgent())\n\n    # Execute a prompt\n    response = await client.execute(\"Explain quantum computing in simple terms\")\n\n    # Print the response\n    print(response[\"response\"])\n\n    # Print which model was used\n    print(f\"Used model: {response['model_id']}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/basic-usage/#using-specific-models","title":"Using Specific Models","text":"<p>Force execution with a specific model:</p> <pre><code># Execute with a specific model\nresponse = await client.execute(\n    \"Explain quantum computing in simple terms\",\n    model_id=\"openai:gpt-3.5-turbo\"\n)\n</code></pre>"},{"location":"examples/basic-usage/#customizing-generation-parameters","title":"Customizing Generation Parameters","text":"<p>Customize parameters for text generation:</p> <pre><code># Execute with custom parameters\nresponse = await client.execute(\n    \"Write a short story about a robot\",\n    temperature=0.8,  # Controls creativity (0.0-1.0)\n    max_tokens=500    # Limits response length\n)\n</code></pre>"},{"location":"examples/basic-usage/#working-with-sessions","title":"Working with Sessions","text":"<p>Create and use sessions for conversational interactions:</p> <pre><code>import asyncio\nfrom oblix import OblixClient, ModelType\n\nasync def main():\n    client = OblixClient(oblix_api_key=\"your_oblix_api_key\")\n\n    # Hook models\n    await client.hook_model(ModelType.OLLAMA, \"llama2\")\n    await client.hook_model(\n        ModelType.OPENAI, \n        \"gpt-3.5-turbo\", \n        api_key=\"your_openai_api_key\"\n    )\n\n    # Create a new session\n    session_id = await client.create_session(title=\"My Conversation\")\n\n    # Set as current session\n    client.current_session_id = session_id\n\n    # Send messages in the session\n    response1 = await client.execute(\"Hello, how are you today?\")\n    print(\"Assistant:\", response1[\"response\"])\n\n    response2 = await client.execute(\"Tell me about yourself\")\n    print(\"Assistant:\", response2[\"response\"])\n\n    # List all sessions\n    sessions = client.list_sessions()\n    print(f\"Number of sessions: {len(sessions)}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/basic-usage/#simplified-chat-interface","title":"Simplified Chat Interface","text":"<p>Use the simplified chat interface for conversational applications:</p> <pre><code>import asyncio\nfrom oblix import OblixClient, ModelType\n\nasync def main():\n    client = OblixClient(oblix_api_key=\"your_oblix_api_key\")\n\n    # Hook models\n    await client.hook_model(ModelType.OLLAMA, \"llama2\")\n    await client.hook_model(\n        ModelType.OPENAI, \n        \"gpt-3.5-turbo\", \n        api_key=\"your_openai_api_key\"\n    )\n\n    # Send a chat message (creates a new session)\n    response = await client.chat_once(\"Hello, how are you today?\")\n    session_id = response[\"session_id\"]\n    print(\"Assistant:\", response[\"response\"])\n\n    # Continue the conversation in the same session\n    response = await client.chat_once(\"What's the weather like?\", session_id)\n    print(\"Assistant:\", response[\"response\"])\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/basic-usage/#interactive-terminal-chat","title":"Interactive Terminal Chat","text":"<p>Start an interactive chat session in the terminal:</p> <pre><code>import asyncio\nfrom oblix import OblixClient, ModelType\n\nasync def main():\n    client = OblixClient(oblix_api_key=\"your_oblix_api_key\")\n\n    # Hook models\n    await client.hook_model(ModelType.OLLAMA, \"llama2\")\n    await client.hook_model(\n        ModelType.OPENAI, \n        \"gpt-3.5-turbo\", \n        api_key=\"your_openai_api_key\"\n    )\n\n    # Start interactive chat session\n    print(\"Starting chat session. Type 'exit' to quit.\")\n    await client.start_chat()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/basic-usage/#error-handling","title":"Error Handling","text":"<p>Handle potential errors gracefully:</p> <pre><code>import asyncio\nfrom oblix import OblixClient, ModelType\n\nasync def main():\n    try:\n        client = OblixClient(oblix_api_key=\"your_oblix_api_key\")\n\n        # Hook models\n        await client.hook_model(ModelType.OLLAMA, \"llama2\")\n\n        # Execute a prompt\n        response = await client.execute(\"Explain quantum computing\")\n        print(response[\"response\"])\n\n    except Exception as e:\n        print(f\"Error: {e}\")\n\n    finally:\n        # Clean up resources\n        if 'client' in locals():\n            await client.shutdown()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/basic-usage/#complete-example","title":"Complete Example","text":"<p>Here's a complete example combining multiple features:</p> <pre><code>import asyncio\nfrom oblix import OblixClient, ModelType\nfrom oblix.agents.resource_monitor import ResourceMonitor\nfrom oblix.agents.connectivity import ConnectivityAgent\n\nasync def main():\n    try:\n        # Initialize client\n        client = OblixClient(oblix_api_key=\"your_oblix_api_key\")\n\n        # Hook models\n        await client.hook_model(ModelType.OLLAMA, \"llama2\")\n        await client.hook_model(\n            ModelType.OPENAI, \n            \"gpt-3.5-turbo\", \n            api_key=\"your_openai_api_key\"\n        )\n\n        # Add monitoring agents\n        client.hook_agent(ResourceMonitor())\n        client.hook_agent(ConnectivityAgent())\n\n        # Create a session\n        session_id = await client.create_session(title=\"AI Assistant Chat\")\n        client.current_session_id = session_id\n\n        # Execute prompts in the session\n        print(\"Asking about quantum computing...\")\n        response1 = await client.execute(\"Explain quantum computing briefly\")\n        print(f\"Response from {response1['model_id']}:\")\n        print(response1[\"response\"])\n        print(\"\\n\" + \"-\"*50 + \"\\n\")\n\n        print(\"Asking a follow-up question...\")\n        response2 = await client.execute(\"What are some practical applications?\")\n        print(f\"Response from {response2['model_id']}:\")\n        print(response2[\"response\"])\n\n        # Print agent check results\n        print(\"\\nAgent check results:\")\n        for agent_name, check_result in response2[\"agent_checks\"].items():\n            print(f\"- {agent_name}: {check_result.get('state')}, target: {check_result.get('target')}\")\n\n    except Exception as e:\n        print(f\"Error: {e}\")\n\n    finally:\n        # Clean up resources\n        if 'client' in locals():\n            await client.shutdown()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>This example demonstrates: - Initializing the client - Hooking multiple models - Adding monitoring agents - Creating and using a session - Executing multiple prompts - Accessing model information and agent checks - Proper error handling and resource cleanup</p>"},{"location":"examples/basic-usage/#macos-specific-considerations","title":"macOS-Specific Considerations","text":"<p>Since Oblix is currently only available for macOS, here are some platform-specific notes:</p> <ul> <li>Oblix leverages macOS-specific optimizations for resource monitoring</li> <li>For Apple Silicon Macs, Oblix can detect and utilize Metal-compatible GPUs</li> <li>The built-in agents are tuned for optimal performance on macOS</li> <li>All examples in this documentation will work on macOS 10.15 (Catalina) or newer</li> </ul> <p>For more advanced examples, see Hybrid Execution and specific use cases in the examples section.</p> <p>Need help? Join our community on Discord: https://discord.gg/v8qtEVuU</p>"},{"location":"examples/hybrid-execution/","title":"Hybrid Execution","text":"<p>This page demonstrates how to implement hybrid execution strategies with Oblix, intelligently routing between local and cloud models based on system resources, connectivity, and other factors.</p>"},{"location":"examples/hybrid-execution/#what-is-hybrid-execution","title":"What is Hybrid Execution?","text":"<p>Hybrid execution refers to the ability to seamlessly switch between:</p> <ul> <li>Local models (via Ollama) running on your own hardware</li> <li>Cloud models (via OpenAI, Claude) running on remote servers</li> </ul> <p>This approach combines the advantages of both:</p> <ul> <li>Local models: Privacy, offline capability, no usage fees</li> <li>Cloud models: Higher capabilities, larger context windows, specialized features</li> </ul>"},{"location":"examples/hybrid-execution/#basic-hybrid-setup","title":"Basic Hybrid Setup","text":"<p>This example shows how to set up a basic hybrid system:</p> <pre><code>import asyncio\nfrom oblix import OblixClient, ModelType\nfrom oblix.agents import ResourceMonitor, ConnectivityAgent\n\nasync def main():\n    # Initialize client\n    client = OblixClient(oblix_api_key=\"your_oblix_api_key\")\n\n    # Hook a local model\n    await client.hook_model(\n        model_type=ModelType.OLLAMA,\n        model_name=\"llama2\"\n    )\n\n    # Hook a cloud model\n    await client.hook_model(\n        model_type=ModelType.OPENAI,\n        model_name=\"gpt-3.5-turbo\",\n        api_key=\"your_openai_api_key\"\n    )\n\n    # Add resource monitoring\n    client.hook_agent(ResourceMonitor())\n\n    # Add connectivity monitoring\n    client.hook_agent(ConnectivityAgent())\n\n    # Execute a prompt (Oblix will automatically choose the appropriate model)\n    response = await client.execute(\"Explain quantum computing in simple terms\")\n\n    print(f\"Response from {response['model_id']}:\")\n    print(response[\"response\"])\n\n    # Print agent check results\n    print(\"\\nAgent check results:\")\n    for agent_name, check_result in response[\"agent_checks\"].items():\n        print(f\"- {agent_name}: {check_result.get('state')}, target: {check_result.get('target')}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/hybrid-execution/#multi-tier-model-strategy","title":"Multi-Tier Model Strategy","text":"<p>For more sophisticated applications, you can implement a multi-tier model strategy:</p> <pre><code>import asyncio\nfrom oblix import OblixClient, ModelType\nfrom oblix.agents import ResourceMonitor, ConnectivityAgent\n\nasync def main():\n    # Initialize client\n    client = OblixClient(oblix_api_key=\"your_oblix_api_key\")\n\n    # Tier 1: Small local model (fast, low resources)\n    await client.hook_model(\n        model_type=ModelType.OLLAMA,\n        model_name=\"phi\"  # or other small model\n    )\n\n    # Tier 2: Medium local model (better quality, more resources)\n    await client.hook_model(\n        model_type=ModelType.OLLAMA,\n        model_name=\"llama2\"\n    )\n\n    # Tier 3: Standard cloud model (good quality, cost-effective)\n    await client.hook_model(\n        model_type=ModelType.OPENAI,\n        model_name=\"gpt-3.5-turbo\",\n        api_key=\"your_openai_api_key\"\n    )\n\n    # Tier 4: Advanced cloud model (highest quality, more expensive)\n    await client.hook_model(\n        model_type=ModelType.OPENAI,\n        model_name=\"gpt-4\",\n        api_key=\"your_openai_api_key\"\n    )\n\n    # Add monitoring agents\n    client.hook_agent(ResourceMonitor())\n    client.hook_agent(ConnectivityAgent())\n\n    # Execute a simple prompt (likely routes to a lower tier)\n    simple_response = await client.execute(\"What is the capital of France?\")\n    print(f\"Simple query used: {simple_response['model_id']}\")\n\n    # Execute a complex prompt (likely routes to a higher tier)\n    complex_response = await client.execute(\n        \"Analyze the implications of quantum computing on modern cryptography, \"\n        \"including potential vulnerabilities in RSA and elliptic curve systems.\"\n    )\n    print(f\"Complex query used: {complex_response['model_id']}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/hybrid-execution/#offline-first-implementation","title":"Offline-First Implementation","text":"<p>This example prioritizes offline capability while falling back to cloud when necessary:</p> <pre><code>import asyncio\nfrom oblix import OblixClient, ModelType\nfrom oblix.agents import ResourceMonitor, ConnectivityAgent\n\nasync def offline_first_app():\n    # Initialize client\n    client = OblixClient(oblix_api_key=\"your_oblix_api_key\")\n\n    # Primary model: Local\n    await client.hook_model(\n        model_type=ModelType.OLLAMA,\n        model_name=\"llama2\"\n    )\n\n    # Fallback model: Cloud (only used when necessary)\n    await client.hook_model(\n        model_type=ModelType.OPENAI,\n        model_name=\"gpt-3.5-turbo\",\n        api_key=\"your_openai_api_key\"\n    )\n\n    # Add connectivity monitoring\n    connectivity_agent = ConnectivityAgent(\n        # Stricter thresholds to prefer local models\n        latency_threshold=100.0,       # Default is 200.0\n        packet_loss_threshold=5.0,     # Default is 10.0\n        bandwidth_threshold=10.0       # Default is 5.0\n    )\n    client.hook_agent(connectivity_agent)\n\n    # Add resource monitoring\n    resource_monitor = ResourceMonitor(\n        custom_thresholds={\n            # More lenient resource thresholds to prefer local models\n            \"cpu_threshold\": 90.0,     # Default is 80.0\n            \"memory_threshold\": 90.0,  # Default is 85.0\n        }\n    )\n    client.hook_agent(resource_monitor)\n\n    # Function to process user input\n    async def process_user_query(query):\n        try:\n            response = await client.execute(query)\n            print(f\"\\nUsing model: {response['model_id']}\")\n            print(f\"Response: {response['response']}\")\n\n            # Check if we're offline\n            connectivity = response[\"agent_checks\"].get(\"connectivity_monitor\", {})\n            if connectivity.get(\"state\") == \"disconnected\":\n                print(\"\\n[OFFLINE MODE ACTIVE]\")\n\n        except Exception as e:\n            print(f\"Error: {e}\")\n            # Even if everything fails, provide minimal functionality\n            print(\"Unable to process request. Working in emergency mode.\")\n\n    # Simple interactive loop\n    print(\"Offline-First AI Assistant (type 'exit' to quit)\")\n    while True:\n        user_input = input(\"\\nYou: \")\n        if user_input.lower() == 'exit':\n            break\n        await process_user_query(user_input)\n\nif __name__ == \"__main__\":\n    asyncio.run(offline_first_app())\n</code></pre>"},{"location":"examples/hybrid-execution/#performance-optimized-hybrid","title":"Performance-Optimized Hybrid","text":"<p>This example optimizes for performance in a hybrid setup:</p> <pre><code>import asyncio\nfrom oblix import OblixClient, ModelType\nfrom oblix.agents import ResourceMonitor, ConnectivityAgent\n\nasync def performance_optimized_app():\n    # Initialize client\n    client = OblixClient(oblix_api_key=\"your_oblix_api_key\")\n\n    # Fast local model for quick responses\n    await client.hook_model(\n        model_type=ModelType.OLLAMA,\n        model_name=\"phi\"  # Small, fast model\n    )\n\n    # Powerful cloud model for complex tasks\n    await client.hook_model(\n        model_type=ModelType.OPENAI,\n        model_name=\"gpt-3.5-turbo\",\n        api_key=\"your_openai_api_key\"\n    )\n\n    # Add resource monitoring\n    resource_monitor = ResourceMonitor()\n    client.hook_agent(resource_monitor)\n\n    # Add connectivity monitoring with performance-oriented thresholds\n    connectivity_agent = ConnectivityAgent(\n        latency_threshold=100.0,  # Lower threshold for better performance\n        check_interval=15         # More frequent checks\n    )\n    client.hook_agent(connectivity_agent)\n\n    # Example task classification function\n    def classify_task_complexity(prompt):\n        # Simple heuristic: longer prompts or certain keywords indicate complexity\n        complexity_keywords = [\n            \"analyze\", \"compare\", \"explain\", \"discuss\", \"evaluate\", \n            \"synthesize\", \"recommend\", \"critique\"\n        ]\n\n        # Check prompt length and keywords\n        is_complex = len(prompt.split()) &gt; 20 or any(kw in prompt.lower() for kw in complexity_keywords)\n        return \"complex\" if is_complex else \"simple\"\n\n    # Process tasks based on complexity\n    async def process_task(prompt):\n        task_type = classify_task_complexity(prompt)\n\n        if task_type == \"simple\":\n            # For simple tasks, explicitly use the faster local model\n            print(\"Simple task detected, using local model for speed\")\n            response = await client.execute(prompt, model_id=\"ollama:phi\")\n        else:\n            # For complex tasks, let Oblix decide based on current conditions\n            print(\"Complex task detected, using intelligent routing\")\n            response = await client.execute(prompt)\n\n        print(f\"Response from {response['model_id']}:\")\n        print(response[\"response\"])\n\n        # Performance metrics\n        metrics = response.get(\"metrics\", {})\n        if metrics:\n            latency = metrics.get(\"total_latency\")\n            if latency:\n                print(f\"Response time: {latency:.2f} seconds\")\n\n    # Example tasks\n    tasks = [\n        \"What is 15 + 27?\",\n        \"What's the capital of Japan?\",\n        \"Explain the theory of relativity and its implications for modern physics, including experimental evidence.\",\n        \"Recommend a strategy for implementing a microservice architecture in a legacy monolithic application.\"\n    ]\n\n    # Process all tasks\n    for task in tasks:\n        print(f\"\\nTask: {task}\")\n        await process_task(task)\n\nif __name__ == \"__main__\":\n    asyncio.run(performance_optimized_app())\n</code></pre>"},{"location":"examples/hybrid-execution/#cost-optimized-hybrid","title":"Cost-Optimized Hybrid","text":"<p>This example optimizes for cost in a hybrid setup:</p> <pre><code>import asyncio\nimport time\nfrom oblix import OblixClient, ModelType\nfrom oblix.agents import ResourceMonitor, ConnectivityAgent\nfrom oblix.agents.base import BaseAgent\nfrom typing import Dict, Any\n\n# Custom agent for cost optimization\nclass CostOptimizationAgent(BaseAgent):\n    def __init__(self, name: str = \"cost_optimizer\", \n                 daily_budget: float = 5.0):\n        super().__init__(name)\n        self.daily_budget = daily_budget\n        self.daily_spend = 0.0\n        self.reset_time = time.time() + 86400  # 24 hours from now\n\n        # Estimated cost per 1K tokens\n        self.cost_per_model = {\n            \"openai:gpt-3.5-turbo\": 0.0015,\n            \"openai:gpt-4\": 0.03,\n            \"claude:claude-3-opus-20240229\": 0.015,\n            \"claude:claude-3-sonnet-20240229\": 0.003,\n            \"ollama:llama2\": 0.0,\n            \"ollama:phi\": 0.0\n        }\n\n    async def initialize(self) -&gt; bool:\n        self.is_active = True\n        return True\n\n    def _estimate_cost(self, model_id: str, input_tokens: int, output_tokens: int) -&gt; float:\n        \"\"\"Estimate cost for a model execution\"\"\"\n        if model_id not in self.cost_per_model:\n            return 0.0\n\n        cost_per_1k = self.cost_per_model[model_id]\n        total_tokens = input_tokens + output_tokens\n        return (total_tokens / 1000) * cost_per_1k\n\n    def _update_spend(self, model_id: str, input_tokens: int, output_tokens: int):\n        \"\"\"Update running spend total\"\"\"\n        # Reset daily budget if needed\n        if time.time() &gt; self.reset_time:\n            self.daily_spend = 0.0\n            self.reset_time = time.time() + 86400\n\n        # Add estimated cost\n        self.daily_spend += self._estimate_cost(model_id, input_tokens, output_tokens)\n\n    async def check(self, **kwargs) -&gt; Dict[str, Any]:\n        # Get prompt info if available\n        prompt = kwargs.get(\"prompt\", \"\")\n\n        # Check budget status\n        budget_percentage = (self.daily_spend / self.daily_budget) * 100\n\n        if budget_percentage &gt; 90:\n            # Critical budget: force local models\n            return {\n                \"proceed\": True,\n                \"state\": \"budget_critical\",\n                \"target\": \"local\",\n                \"reason\": f\"Budget nearly depleted: ${self.daily_spend:.2f}/${self.daily_budget:.2f}\",\n                \"metrics\": {\n                    \"budget_percentage\": budget_percentage,\n                    \"daily_spend\": self.daily_spend,\n                    \"daily_budget\": self.daily_budget\n                }\n            }\n        elif budget_percentage &gt; 70:\n            # High budget use: prefer local but allow cloud for complex tasks\n            return {\n                \"proceed\": True,\n                \"state\": \"budget_constrained\",\n                \"target\": \"hybrid\",\n                \"reason\": f\"Budget usage high: ${self.daily_spend:.2f}/${self.daily_budget:.2f}\",\n                \"metrics\": {\n                    \"budget_percentage\": budget_percentage,\n                    \"daily_spend\": self.daily_spend,\n                    \"daily_budget\": self.daily_budget\n                }\n            }\n        else:\n            # Budget available: make recommendations based on estimated complexity\n            # Simple heuristic: longer prompts may need more powerful models\n            is_complex = len(prompt.split()) &gt; 25\n\n            return {\n                \"proceed\": True,\n                \"state\": \"budget_available\",\n                \"target\": \"cloud\" if is_complex else \"local\",\n                \"reason\": f\"Budget available: ${self.daily_spend:.2f}/${self.daily_budget:.2f}\",\n                \"metrics\": {\n                    \"budget_percentage\": budget_percentage,\n                    \"daily_spend\": self.daily_spend,\n                    \"daily_budget\": self.daily_budget,\n                    \"estimated_complexity\": \"complex\" if is_complex else \"simple\"\n                }\n            }\n\n    async def shutdown(self) -&gt; None:\n        self.is_active = False\n\nasync def cost_optimized_app():\n    # Initialize client\n    client = OblixClient(oblix_api_key=\"your_oblix_api_key\")\n\n    # Hook models\n    # Tier 1: Free local model (no cost)\n    await client.hook_model(\n        model_type=ModelType.OLLAMA,\n        model_name=\"llama2\"\n    )\n\n    # Tier 2: Economy cloud model (low cost)\n    await client.hook_model(\n        model_type=ModelType.OPENAI,\n        model_name=\"gpt-3.5-turbo\",\n        api_key=\"your_openai_api_key\"\n    )\n\n    # Tier 3: Premium cloud model (higher cost, only used when necessary)\n    await client.hook_model(\n        model_type=ModelType.OPENAI,\n        model_name=\"gpt-4\",\n        api_key=\"your_openai_api_key\"\n    )\n\n    # Add standard monitoring agents\n    client.hook_agent(ResourceMonitor())\n    client.hook_agent(ConnectivityAgent())\n\n    # Add cost optimization agent\n    cost_agent = CostOptimizationAgent(daily_budget=5.0)  # $5 daily budget\n    client.hook_agent(cost_agent)\n\n    # Test with various queries\n    test_queries = [\n        \"What time is it?\",  # Very simple\n        \"Explain how photosynthesis works\",  # Moderate\n        \"Write a detailed analysis of sustainable energy solutions for developing nations\",  # Complex\n    ]\n\n    for query in test_queries:\n        print(f\"\\n--- Query: {query}\")\n\n        # Execute the query\n        response = await client.execute(query)\n\n        # Print model used and cost info\n        model_id = response[\"model_id\"]\n        print(f\"Model used: {model_id}\")\n\n        # Update cost tracker with token usage\n        metrics = response.get(\"metrics\", {})\n        input_tokens = metrics.get(\"input_tokens\", len(query.split()) * 1.3)  # Rough estimate if not available\n        output_tokens = metrics.get(\"output_tokens\", len(response[\"response\"].split()) * 1.3)\n\n        cost_agent._update_spend(model_id, input_tokens, output_tokens)\n\n        # Print budget status\n        budget_percentage = (cost_agent.daily_spend / cost_agent.daily_budget) * 100\n        print(f\"Budget status: ${cost_agent.daily_spend:.4f} spent (of ${cost_agent.daily_budget:.2f} total)\")\n        print(f\"Budget usage: {budget_percentage:.1f}%\")\n\n        # Abbreviated response\n        response_text = response[\"response\"]\n        print(f\"Response: {response_text[:100]}...\" if len(response_text) &gt; 100 else response_text)\n\n    print(\"\\n--- Final Budget Status ---\")\n    print(f\"Total spend: ${cost_agent.daily_spend:.4f}\")\n    print(f\"Remaining budget: ${cost_agent.daily_budget - cost_agent.daily_spend:.4f}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(cost_optimized_app())\n</code></pre>"},{"location":"examples/hybrid-execution/#conclusion","title":"Conclusion","text":"<p>Hybrid execution with Oblix provides several key benefits:</p> <ol> <li>Resilience: Continue operation even without internet connectivity</li> <li>Cost optimization: Use free local models when possible, cloud models when necessary</li> <li>Performance optimization: Route to the most appropriate model based on the task</li> <li>Privacy: Keep sensitive prompts on local models</li> <li>Adaptability: Automatically adjust to changing conditions</li> </ol> <p>By implementing these patterns, you can build AI applications that are more robust, cost-effective, and performant than those using only local or only cloud models.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Welcome to Oblix! This section will guide you through the process of installing the SDK, setting up your environment, and creating your first Oblix application.</p>"},{"location":"getting-started/#what-is-oblix","title":"What is Oblix?","text":"<p>Oblix is a comprehensive SDK for orchestrating AI models with automatic switching between local and cloud-based models. The SDK provides:</p> <ul> <li>Unified interface for multiple AI model providers (OpenAI, Claude, Ollama)</li> <li>Intelligent orchestration between local and cloud models</li> <li>System resource monitoring and connectivity awareness</li> <li>Persistent chat session management</li> <li>Agent system for extensible monitoring and decision making</li> </ul>"},{"location":"getting-started/#installation","title":"Installation","text":"<p>Oblix is currently available for macOS only and is distributed as a .dmg installer file:</p> <ol> <li>Download the Oblix installer (.dmg) from Oblix AI</li> <li>Open the .dmg file</li> <li>Drag the Oblix application to your Applications folder</li> <li>Launch Oblix to complete the installation</li> </ol>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<p>Before using Oblix, you'll need:</p> <ol> <li>macOS - Oblix currently only supports macOS</li> <li>Oblix API Key for authentication (see Authentication)</li> <li>Provider API Keys for any cloud models you want to use</li> <li>Ollama installed if you want to use local models</li> </ol>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<p>To continue getting started with Oblix, check out these guides:</p> <ul> <li>Quickstart - A more comprehensive quickstart guide</li> <li>Authentication - Setting up authentication</li> </ul> <p>Once you're set up, you can explore the core concepts behind Oblix and the API reference for detailed information about the available functions and classes.</p>"},{"location":"getting-started/authentication/","title":"Authentication","text":"<p>To use the Oblix SDK, you'll need to authenticate using an Oblix API key. This guide explains how to obtain and use your API key.</p>"},{"location":"getting-started/authentication/#getting-an-api-key","title":"Getting an API Key","text":"<ol> <li>Create an account at Oblix AI</li> <li>Navigate to the API Keys section in your dashboard</li> <li>Click \"Create New API Key\"</li> <li>Copy your new API key to a secure location</li> </ol> <p>API Key Security</p> <p>Your Oblix API key grants access to the Oblix platform and should be kept secure. Never share your API key publicly or commit it to version control systems.</p>"},{"location":"getting-started/authentication/#using-your-api-key","title":"Using Your API Key","text":"<p>There are two ways to provide your API key to the Oblix SDK:</p>"},{"location":"getting-started/authentication/#1-direct-initialization","title":"1. Direct Initialization","text":"<p>Pass your API key directly when initializing the client:</p> <pre><code>from oblix import OblixClient\n\nclient = OblixClient(oblix_api_key=\"your_api_key_here\")\n</code></pre>"},{"location":"getting-started/authentication/#2-environment-variable","title":"2. Environment Variable","text":"<p>Set the <code>OBLIX_API_KEY</code> environment variable:</p> <pre><code># macOS\nexport OBLIX_API_KEY=your_api_key_here\n</code></pre> <p>Then initialize the client without specifying the API key:</p> <pre><code>from oblix import OblixClient\n\n# API key will be automatically loaded from environment variable\nclient = OblixClient()\n</code></pre>"},{"location":"getting-started/authentication/#provider-api-keys","title":"Provider API Keys","text":"<p>In addition to your Oblix API key, you'll need provider-specific API keys for any cloud models you want to use:</p>"},{"location":"getting-started/authentication/#openai-api-key","title":"OpenAI API Key","text":"<p>For using OpenAI models (like GPT-3.5 or GPT-4), you'll need to provide your OpenAI API key when hooking the model:</p> <pre><code>from oblix import OblixClient, ModelType\n\nclient = OblixClient(oblix_api_key=\"your_oblix_api_key\")\n\n# Hook an OpenAI model with your OpenAI API key\nawait client.hook_model(\n    model_type=ModelType.OPENAI,\n    model_name=\"gpt-3.5-turbo\",\n    api_key=\"your_openai_api_key\"\n)\n</code></pre>"},{"location":"getting-started/authentication/#anthropic-api-key","title":"Anthropic API Key","text":"<p>For using Claude models, you'll need to provide your Anthropic API key:</p> <pre><code>from oblix import OblixClient, ModelType\n\nclient = OblixClient(oblix_api_key=\"your_oblix_api_key\")\n\n# Hook a Claude model with your Anthropic API key\nawait client.hook_model(\n    model_type=ModelType.CLAUDE,\n    model_name=\"claude-3-opus-20240229\",\n    api_key=\"your_anthropic_api_key\"\n)\n</code></pre>"},{"location":"getting-started/authentication/#ollama-local-models","title":"Ollama (Local Models)","text":"<p>For Ollama models, no API key is required, but you'll need to specify the endpoint if you're not using the default:</p> <pre><code>from oblix import OblixClient, ModelType\n\nclient = OblixClient(oblix_api_key=\"your_oblix_api_key\")\n\n# Hook an Ollama model\nawait client.hook_model(\n    model_type=ModelType.OLLAMA,\n    model_name=\"llama2\",\n    endpoint=\"http://localhost:11434\"  # Optional, this is the default\n)\n</code></pre>"},{"location":"getting-started/authentication/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter authentication issues:</p> <ol> <li>Verify that your API key is correct</li> <li>Ensure you have an active Oblix subscription</li> <li>Check that your API key has not been revoked</li> <li>For provider-specific issues, verify your provider API keys</li> </ol> <p>For persistent issues, contact Oblix support at team@oblix.ai.</p>"},{"location":"getting-started/installation/","title":"Installation Guide","text":"<p>This guide walks you through the process of installing Oblix on your macOS system.</p>"},{"location":"getting-started/installation/#system-requirements","title":"System Requirements","text":"<ul> <li>Operating System: macOS 11 (Big Sur) or newer</li> <li>Storage: 800 MB of free disk space</li> <li>Memory: 8 GB RAM recommended</li> <li>Internet Connection: Required for cloud model access</li> </ul>"},{"location":"getting-started/installation/#installation-steps","title":"Installation Steps","text":"<ol> <li>Download the Installer: </li> <li>Visit Oblix AI</li> <li> <p>Download the latest Oblix installer (.dmg file)</p> </li> <li> <p>Open the Installer:</p> </li> <li>Locate the downloaded .dmg file in your Downloads folder</li> <li>Double-click the .dmg file to mount it</li> <li> <p>A new Finder window will open showing the Oblix application and Applications folder</p> </li> <li> <p>Install Oblix:</p> </li> <li>Drag the Oblix application icon to the Applications folder</li> <li> <p>Close the installer window</p> </li> <li> <p>Run Oblix for the First Time:</p> </li> <li>Navigate to your Applications folder</li> <li>Double-click the Oblix application</li> <li>macOS may display a security warning the first time you open it</li> <li> <p>Click \"Open\" if prompted</p> </li> <li> <p>Complete Installation:</p> </li> <li>You'll be prompted to enter your system password</li> <li>This is required to install necessary components</li> <li>After entering your password, the installation will complete</li> <li> <p>You should see a confirmation message that Oblix has been successfully installed</p> </li> <li> <p>Verify Installation:</p> </li> <li>Open Terminal</li> <li>Run the following command to verify Oblix is properly installed:      <pre><code>oblix --version\n</code></pre></li> <li>You should see the current version of Oblix displayed</li> </ol>"},{"location":"getting-started/installation/#using-oblix","title":"Using Oblix","text":"<p>After installation, you'll need to set up environment variables before using Oblix from the Terminal.</p>"},{"location":"getting-started/installation/#required-environment-variables","title":"Required Environment Variables","text":"<p>For Oblix to function properly, you must set the following environment variables:</p> <ol> <li>OBLIX_API_KEY - Your Oblix API key</li> <li>OPENAI_API_KEY - Your OpenAI API key (for using GPT models)</li> <li>ANTHROPIC_API_KEY - Your Anthropic API key (for using Claude models)</li> </ol> <p>You can set these in your terminal session or add them to your shell profile for persistence:</p> <pre><code># Add these lines to your ~/.zshrc or ~/.bash_profile\nexport OBLIX_API_KEY=\"your_oblix_api_key_here\"\nexport OPENAI_API_KEY=\"your_openai_api_key_here\"\nexport ANTHROPIC_API_KEY=\"your_anthropic_api_key_here\"\n</code></pre> <p>After adding these to your profile file, apply the changes:</p> <pre><code>source ~/.zshrc  # or source ~/.bash_profile\n</code></pre>"},{"location":"getting-started/installation/#using-the-cli","title":"Using the CLI","text":"<p>Once environment variables are set, you can use Oblix from the Terminal:</p> <pre><code># Get help with available commands\noblix --help\n\n# Start an interactive chat session\noblix chat --local-model llama2 --cloud-model gpt-3.5-turbo\n</code></pre> <p>You can also provide API keys directly in commands:</p> <pre><code>oblix chat --local-model llama2 --cloud-model gpt-3.5-turbo --cloud-api-key \"your_api_key_here\"\n</code></pre>"},{"location":"getting-started/installation/#installing-ollama-required-for-local-models","title":"Installing Ollama (Required for Local Models)","text":"<p>To use local models with Oblix, you'll need to install Ollama:</p> <ol> <li>Download Ollama from ollama.ai</li> <li>Follow the installation instructions for macOS</li> <li>Verify Ollama is running by opening Terminal and typing:    <pre><code>ollama list\n</code></pre></li> </ol>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#common-issues","title":"Common Issues","text":"<p>Issue: \"Oblix can't be opened because it is from an unidentified developer\" Solution: Go to System Preferences &gt; Security &amp; Privacy &gt; General, and click \"Open Anyway\"</p> <p>Issue: Command not found when trying to use Oblix in Terminal Solution: Make sure the installation completed successfully. You might need to restart your Terminal or add Oblix to your PATH.</p> <p>Issue: System password prompt appears multiple times Solution: This is normal behavior if you cancel the password prompt. Enter your password to continue installation.</p>"},{"location":"getting-started/installation/#getting-help","title":"Getting Help","text":"<p>If you encounter issues during installation:</p> <ol> <li>Join our Discord community: https://discord.gg/v8qtEVuU</li> <li>Contact support at team@oblix.ai</li> </ol>"},{"location":"getting-started/installation/#uninstallation","title":"Uninstallation","text":"<p>To uninstall Oblix:</p> <ol> <li>Open Applications folder in Finder</li> <li>Drag Oblix to the Trash</li> <li>Empty the Trash</li> <li>Optional: Remove configuration files with:    <pre><code>rm -rf ~/.oblix\n</code></pre></li> </ol>"},{"location":"getting-started/quickstart/","title":"Quickstart Guide","text":"<p>Get started with Oblix, an AI orchestration SDK for seamless switching between local and cloud models based on connectivity and system resources.</p>"},{"location":"getting-started/quickstart/#installation","title":"Installation","text":"<ol> <li>Download the Oblix installer (.dmg) from Oblix AI</li> <li>Open the .dmg file</li> <li>Drag the Oblix application to your Applications folder</li> <li>Launch Oblix to complete the installation</li> </ol> <p>Note: Oblix is currently available for macOS only.</p>"},{"location":"getting-started/quickstart/#basic-usage","title":"Basic Usage","text":""},{"location":"getting-started/quickstart/#1-initialize-client","title":"1. Initialize Client","text":"<pre><code>from oblix import OblixClient\n\n# Initialize the client with your Oblix API key\nclient = OblixClient(oblix_api_key=\"your_api_key\")\n</code></pre>"},{"location":"getting-started/quickstart/#2-hook-models","title":"2. Hook Models","text":"<p>Connect at least one local and one cloud model:</p> <pre><code>from oblix import ModelType\n\n# Hook a local Ollama model\nawait client.hook_model(\n    model_type=ModelType.OLLAMA, \n    model_name=\"llama2\"\n)\n\n# Hook a cloud model (OpenAI)\nawait client.hook_model(\n    model_type=ModelType.OPENAI, \n    model_name=\"gpt-3.5-turbo\", \n    api_key=\"your_openai_api_key\"\n)\n</code></pre>"},{"location":"getting-started/quickstart/#3-add-monitoring-agents","title":"3. Add Monitoring Agents","text":"<pre><code>from oblix.agents import ResourceMonitor, ConnectivityAgent\n\n# Add resource monitoring\nclient.hook_agent(ResourceMonitor())\n\n# Add connectivity monitoring\nclient.hook_agent(ConnectivityAgent())\n</code></pre>"},{"location":"getting-started/quickstart/#4-execute-prompts","title":"4. Execute Prompts","text":"<pre><code># Execute a prompt - Oblix automatically orchestrates the optimal model\nresponse = await client.execute(\"Explain quantum computing in simple terms\")\nprint(response[\"response\"])\n</code></pre>"},{"location":"getting-started/quickstart/#5-interactive-chat-session","title":"5. Interactive Chat Session","text":"<pre><code># Start an interactive chat session\nawait client.start_chat()\n</code></pre>"},{"location":"getting-started/quickstart/#example-complete-hybrid-setup","title":"Example: Complete Hybrid Setup","text":"<pre><code>import asyncio\nfrom oblix import OblixClient, ModelType\nfrom oblix.agents import ResourceMonitor, ConnectivityAgent\n\nasync def main():\n    # Initialize client\n    client = OblixClient(oblix_api_key=\"your_oblix_api_key\")\n\n    # Hook models\n    await client.hook_model(ModelType.OLLAMA, \"llama2\")\n    await client.hook_model(ModelType.OPENAI, \"gpt-3.5-turbo\", api_key=\"your_openai_api_key\")\n\n    # Add monitoring agents\n    client.hook_agent(ResourceMonitor())\n    client.hook_agent(ConnectivityAgent())\n\n    # Execute a prompt\n    response = await client.execute(\"Explain quantum computing in simple terms\")\n    print(response[\"response\"])\n\n    # Start interactive chat\n    await client.start_chat()\n\n# Run the example\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"getting-started/quickstart/#whats-next","title":"What's Next?","text":"<ul> <li>Learn about how Oblix works</li> <li>Explore examples</li> <li>Set up different model providers</li> <li>Check out the API reference</li> </ul>"},{"location":"providers/","title":"Model Providers","text":"<p>Oblix supports multiple AI model providers, allowing you to seamlessly switch between local and cloud models based on your needs. This page provides an overview of the supported providers and how to configure them.</p>"},{"location":"providers/#supported-providers","title":"Supported Providers","text":"<p>Oblix currently supports the following model providers:</p> Provider Type Description Key Features Ollama Local Run open-source models locally Privacy, offline capability, no usage fees OpenAI Cloud Access GPT models via API High capability, extensive features Claude Cloud Access Anthropic's Claude models Long context windows, reasoning capabilities"},{"location":"providers/#provider-configuration","title":"Provider Configuration","text":"<p>Each provider requires specific configuration when hooking models:</p> <pre><code>from oblix import OblixClient, ModelType\n\nclient = OblixClient(oblix_api_key=\"your_oblix_api_key\")\n\n# Hook Ollama (local) model\nawait client.hook_model(\n    model_type=ModelType.OLLAMA,\n    model_name=\"llama2\",\n    endpoint=\"http://localhost:11434\"  # Optional, this is the default\n)\n\n# Hook OpenAI (cloud) model\nawait client.hook_model(\n    model_type=ModelType.OPENAI,\n    model_name=\"gpt-3.5-turbo\",\n    api_key=\"your_openai_api_key\"\n)\n\n# Hook Claude (cloud) model\nawait client.hook_model(\n    model_type=ModelType.CLAUDE,\n    model_name=\"claude-3-opus-20240229\",\n    api_key=\"your_anthropic_api_key\"\n)\n</code></pre>"},{"location":"providers/#multi-provider-strategy","title":"Multi-Provider Strategy","text":"<p>For optimal results, Oblix recommends configuring at least:</p> <ol> <li>One local model (via Ollama) for offline capability and privacy-sensitive tasks</li> <li>One cloud model (OpenAI or Claude) for more demanding tasks</li> </ol> <p>This hybrid approach enables Oblix to intelligently route prompts based on:</p> <ul> <li>Current connectivity status</li> <li>System resource availability</li> <li>Task complexity requirements</li> </ul>"},{"location":"providers/#provider-selection-process","title":"Provider Selection Process","text":"<p>When executing a prompt, Oblix selects the provider based on:</p> <ol> <li>Explicit selection: If a specific model is requested</li> <li>Agent recommendations: Based on resource and connectivity checks</li> <li>Default fallback: Using the first available model</li> </ol> <p>You can see which provider was used in the response:</p> <pre><code>response = await client.execute(\"Explain quantum computing\")\nprint(f\"Used provider/model: {response['model_id']}\")\n</code></pre>"},{"location":"providers/#adding-custom-providers","title":"Adding Custom Providers","text":"<p>Oblix can be extended to support additional model providers by:</p> <ol> <li>Creating a custom model implementation that extends <code>BaseModel</code></li> <li>Implementing required methods (initialize, generate, shutdown)</li> <li>Registering the model with the appropriate <code>ModelType</code></li> </ol>"},{"location":"providers/#provider-specific-documentation","title":"Provider-Specific Documentation","text":"<p>For detailed information about each provider, including supported models, configuration options, and best practices, see:</p> <ul> <li>Ollama - Local models</li> <li>OpenAI - GPT models</li> <li>Claude - Claude models</li> </ul>"},{"location":"providers/claude/","title":"Anthropic Claude","text":"<p>Anthropic is an AI safety and research company, and is the creator of <code>Claude</code>. This page covers how to use Claude models with Oblix for intelligent model orchestration.</p>"},{"location":"providers/claude/#installation-and-setup","title":"Installation and Setup","text":"<p>Oblix is available exclusively for macOS at this time:</p> <ol> <li>Visit oblix.ai to download the latest version</li> <li>Follow the installation instructions on the website</li> <li>Once installed, you can import the SDK in your Python projects</li> </ol> <p>Note: Currently, only macOS is supported. Windows and Linux versions are planned for future releases.</p> <p>You'll need an Anthropic API key to use Claude models with Oblix. You can obtain one from the Anthropic Console.</p>"},{"location":"providers/claude/#hooking-claude-models","title":"Hooking Claude Models","text":"<pre><code>from oblix import OblixClient, ModelType\n\n# Initialize client\nclient = OblixClient(oblix_api_key=\"your_oblix_api_key\")\n\n# Hook Claude model\nawait client.hook_model(\n    model_type=ModelType.CLAUDE,\n    model_name=\"claude-3-opus-20240229\",\n    api_key=\"your_anthropic_api_key\"\n)\n</code></pre>"},{"location":"providers/claude/#supported-claude-models","title":"Supported Claude Models","text":"<p>Oblix supports all current Claude models:</p> <ul> <li><code>claude-3-opus-20240229</code> - Most powerful Claude model with 200K context window</li> <li><code>claude-3-sonnet-20240229</code> - Balanced performance Claude model with 200K context window</li> <li><code>claude-3-haiku-20240307</code> - Fastest Claude model with 200K context window</li> </ul>"},{"location":"providers/claude/#using-claude-with-oblix","title":"Using Claude with Oblix","text":""},{"location":"providers/claude/#basic-execution","title":"Basic Execution","text":"<pre><code>from oblix import OblixClient, ModelType\n\n# Initialize client\nclient = OblixClient(oblix_api_key=\"your_oblix_api_key\")\n\n# Hook Claude model\nawait client.hook_model(\n    model_type=ModelType.CLAUDE,\n    model_name=\"claude-3-opus-20240229\",\n    api_key=\"your_anthropic_api_key\"\n)\n\n# Execute prompt using Claude\nresponse = await client.execute(\n    \"Explain quantum computing in simple terms\",\n    model_id=\"claude:claude-3-opus-20240229\"  # Optionally specify model\n)\n\nprint(response[\"response\"])\n</code></pre>"},{"location":"providers/claude/#hybrid-local-cloud-setup-with-claude","title":"Hybrid Local-Cloud Setup with Claude","text":"<pre><code>from oblix import OblixClient, ModelType\nfrom oblix.agents.resource_monitor import ResourceMonitor\nfrom oblix.agents.connectivity import ConnectivityAgent\n\n# Initialize client\nclient = OblixClient(oblix_api_key=\"your_oblix_api_key\")\n\n# Hook local Ollama model\nawait client.hook_model(\n    model_type=ModelType.OLLAMA,\n    model_name=\"llama2\",\n    endpoint=\"http://localhost:11434\"\n)\n\n# Hook cloud Claude model\nawait client.hook_model(\n    model_type=ModelType.CLAUDE,\n    model_name=\"claude-3-haiku-20240307\",\n    api_key=\"your_anthropic_api_key\"\n)\n\n# Add monitoring agents for intelligent orchestration\nclient.hook_agent(ResourceMonitor())\nclient.hook_agent(ConnectivityAgent())\n\n# Execute prompt - Oblix will automatically choose between \n# local and cloud models based on connectivity and system resources\nresponse = await client.execute(\"Explain quantum computing in simple terms\")\nprint(response[\"response\"])\n</code></pre>"},{"location":"providers/claude/#advanced-configuration","title":"Advanced Configuration","text":"<p>You can customize Claude model settings when executing prompts:</p> <pre><code>response = await client.execute(\n    \"Write a story about a robot who gains consciousness\",\n    model_id=\"claude:claude-3-opus-20240229\",\n    temperature=0.8,\n    max_tokens=2000\n)\n</code></pre>"},{"location":"providers/claude/#supported-parameters","title":"Supported Parameters","text":"<p>Claude models in Oblix support all standard Anthropic parameters:</p> Parameter Description Default <code>temperature</code> Controls randomness (0-1) 0.7 <code>max_tokens</code> Maximum response length 4096 <code>top_p</code> Nucleus sampling parameter 1.0"},{"location":"providers/claude/#performance-considerations","title":"Performance Considerations","text":"<p>When using Claude models with Oblix, consider:</p>"},{"location":"providers/claude/#model-selection","title":"Model Selection","text":"<ul> <li>Use <code>claude-3-haiku</code> for faster responses and routine tasks</li> <li>Use <code>claude-3-sonnet</code> for balanced performance and quality</li> <li>Reserve <code>claude-3-opus</code> for complex reasoning and highest quality outputs</li> </ul>"},{"location":"providers/claude/#orchestration-benefits","title":"Orchestration Benefits","text":"<ul> <li>Oblix's connectivity monitoring helps manage Claude API latency</li> <li>Automatic fallback to local models when connectivity is limited</li> <li>Intelligent model selection based on task complexity and resource availability</li> </ul>"},{"location":"providers/claude/#macos-integration","title":"macOS Integration","text":"<ul> <li>Oblix is optimized specifically for macOS system resource monitoring</li> <li>The platform provides smooth integration between local Ollama models and cloud-based Claude models</li> <li>Takes advantage of macOS performance metrics for intelligent orchestration decisions</li> </ul>"},{"location":"providers/claude/#error-handling","title":"Error Handling","text":"<p>Oblix provides robust error handling for Claude API interactions:</p> <pre><code>try:\n    response = await client.execute(\"Generate a complex analysis\", model_id=\"claude:claude-3-opus-20240229\")\nexcept Exception as e:\n    print(f\"Error: {e}\")\n    # Oblix will automatically try to use a fallback model if available\n</code></pre>"},{"location":"providers/claude/#api-reference","title":"API Reference","text":"<p>See the OblixClient API reference for detailed information on all available methods.</p>"},{"location":"providers/ollama/","title":"Ollama","text":"<p>Ollama is an open-source tool that allows you to run open-source large language models locally on your own hardware. This page covers how to use Ollama models with Oblix for intelligent model orchestration.</p>"},{"location":"providers/ollama/#installation-and-setup","title":"Installation and Setup","text":""},{"location":"providers/ollama/#1-install-oblix","title":"1. Install Oblix","text":"<p>Oblix is available exclusively for macOS at this time:</p> <ol> <li>Visit oblix.ai to download the latest version</li> <li>Follow the installation instructions on the website</li> <li>Once installed, you can import the SDK in your Python projects</li> </ol> <p>Note: Currently, only macOS is supported. Windows and Linux versions are planned for future releases.</p>"},{"location":"providers/ollama/#2-install-ollama","title":"2. Install Ollama","text":"<p>Before using Ollama with Oblix, you need to install Ollama on your Mac:</p> <pre><code>curl -fsSL https://ollama.com/install.sh | sh\n</code></pre>"},{"location":"providers/ollama/#3-start-ollama-server","title":"3. Start Ollama Server","text":"<p>Once installed, make sure the Ollama server is running:</p> <pre><code>ollama serve\n</code></pre> <p>This starts the Ollama server on the default port <code>11434</code>.</p>"},{"location":"providers/ollama/#4-pull-models","title":"4. Pull Models","text":"<p>Pull the models you want to use:</p> <pre><code># Examples of models you can pull\nollama pull llama2\nollama pull mistral\nollama pull gemma\n</code></pre>"},{"location":"providers/ollama/#hooking-ollama-models-in-oblix","title":"Hooking Ollama Models in Oblix","text":"<pre><code>from oblix import OblixClient, ModelType\n\n# Initialize client\nclient = OblixClient(oblix_api_key=\"your_oblix_api_key\")\n\n# Hook Ollama model with default endpoint (http://localhost:11434)\nawait client.hook_model(\n    model_type=ModelType.OLLAMA,\n    model_name=\"llama2\"\n)\n\n# Or specify a custom endpoint\nawait client.hook_model(\n    model_type=ModelType.OLLAMA,\n    model_name=\"llama2\",\n    endpoint=\"http://custom-server:11434\"\n)\n</code></pre>"},{"location":"providers/ollama/#supported-ollama-models","title":"Supported Ollama Models","text":"<p>Ollama supports a wide range of open-source models, including:</p> Model Family Examples Description Llama llama2, llama3 Meta's Llama models Mistral mistral, mixtral Mistral AI's models Phi phi-2 Microsoft's Phi models Gemma gemma:2b, gemma:7b Google's Gemma models Vicuna vicuna Fine-tuned Llama models Orca orca-mini Microsoft's Orca models Falcon falcon TII's Falcon models <p>For the most up-to-date list of available models, see the Ollama model library.</p>"},{"location":"providers/ollama/#using-ollama-with-oblix","title":"Using Ollama with Oblix","text":""},{"location":"providers/ollama/#basic-execution","title":"Basic Execution","text":"<pre><code>from oblix import OblixClient, ModelType\n\n# Initialize client\nclient = OblixClient(oblix_api_key=\"your_oblix_api_key\")\n\n# Hook Ollama model\nawait client.hook_model(\n    model_type=ModelType.OLLAMA,\n    model_name=\"llama2\"\n)\n\n# Execute prompt using Ollama\nresponse = await client.execute(\n    \"Explain quantum computing in simple terms\",\n    model_id=\"ollama:llama2\"  # Optionally specify model\n)\n\nprint(response[\"response\"])\n</code></pre>"},{"location":"providers/ollama/#hybrid-local-cloud-setup-with-ollama","title":"Hybrid Local-Cloud Setup with Ollama","text":"<pre><code>from oblix import OblixClient, ModelType\nfrom oblix.agents.resource_monitor import ResourceMonitor\nfrom oblix.agents.connectivity import ConnectivityAgent\n\n# Initialize client\nclient = OblixClient(oblix_api_key=\"your_oblix_api_key\")\n\n# Hook local Ollama model\nawait client.hook_model(\n    model_type=ModelType.OLLAMA,\n    model_name=\"llama2\"\n)\n\n# Hook cloud model\nawait client.hook_model(\n    model_type=ModelType.OPENAI,\n    model_name=\"gpt-3.5-turbo\",\n    api_key=\"your_openai_api_key\"\n)\n\n# Add monitoring agents for intelligent orchestration\nclient.hook_agent(ResourceMonitor())\nclient.hook_agent(ConnectivityAgent())\n\n# Execute prompt - Oblix will automatically choose between \n# local and cloud models based on connectivity and system resources\nresponse = await client.execute(\"Explain quantum computing in simple terms\")\nprint(response[\"response\"])\n</code></pre>"},{"location":"providers/ollama/#advanced-configuration","title":"Advanced Configuration","text":"<p>You can customize Ollama model settings when executing prompts:</p> <pre><code>response = await client.execute(\n    \"Write a story about a robot who gains consciousness\",\n    model_id=\"ollama:llama2\",\n    temperature=0.8,\n    max_tokens=2000,\n    use_gpu=True  # Enable GPU acceleration if available\n)\n</code></pre>"},{"location":"providers/ollama/#supported-parameters","title":"Supported Parameters","text":"<p>Ollama models in Oblix support the following parameters:</p> Parameter Description Default <code>temperature</code> Controls randomness (0-2) 0.7 <code>max_tokens</code> Maximum response length model-dependent <code>use_gpu</code> Enable GPU acceleration False <code>stop</code> List of stop sequences None"},{"location":"providers/ollama/#performance-considerations","title":"Performance Considerations","text":"<p>When using Ollama models with Oblix, consider:</p>"},{"location":"providers/ollama/#system-requirements-for-macos","title":"System Requirements for macOS","text":"<p>Models have different resource requirements on Mac:</p> Model Size RAM Required Disk Space Apple Silicon Benefits 7B 8GB+ ~4GB Good performance on M1/M2 13B 16GB+ ~8GB Better with M1 Pro/Max/Ultra or M2 70B 32GB+ ~40GB Best with M1/M2 Max/Ultra"},{"location":"providers/ollama/#apple-silicon-optimization","title":"Apple Silicon Optimization","text":"<p>Ollama is optimized for Apple Silicon, and Oblix leverages this effectively:</p> <ul> <li>Metal API: Hardware acceleration on Apple Silicon chips</li> <li>Memory management: Optimized for macOS memory architecture</li> <li>Resource monitoring: Oblix's ResourceMonitor is specially tuned for macOS</li> </ul>"},{"location":"providers/ollama/#offline-capability","title":"Offline Capability","text":"<p>One of the key advantages of Ollama models is offline capability:</p> <ul> <li>Models run completely locally without internet connectivity</li> <li>Perfect for privacy-sensitive applications</li> <li>Ideal for environments with unreliable connections</li> </ul>"},{"location":"providers/ollama/#oblix-orchestration-benefits","title":"Oblix Orchestration Benefits","text":"<p>The combination of Ollama and Oblix provides unique advantages:</p> <ul> <li>Intelligent switching between local Ollama models and cloud models</li> <li>Automatic detection of connectivity status and resource availability</li> <li>Optimization for macOS-specific hardware features</li> <li>Seamless fallback to local models when offline</li> </ul>"},{"location":"providers/ollama/#troubleshooting","title":"Troubleshooting","text":"<p>Common issues and their solutions:</p>"},{"location":"providers/ollama/#model-not-found","title":"Model Not Found","text":"<p>If you encounter \"model not found\" errors:</p> <pre><code># Pull the model manually first\nollama pull llama2\n</code></pre>"},{"location":"providers/ollama/#port-issues","title":"Port Issues","text":"<p>If Ollama is running on a non-standard port:</p> <pre><code>await client.hook_model(\n    model_type=ModelType.OLLAMA,\n    model_name=\"llama2\",\n    endpoint=\"http://localhost:YOUR_PORT\"\n)\n</code></pre>"},{"location":"providers/ollama/#resource-constraints-on-mac","title":"Resource Constraints on Mac","text":"<p>If the model is running slowly or crashing on your Mac:</p> <ul> <li>Try a smaller model (e.g., switch from llama2:13b to llama2:7b)</li> <li>Ensure adequate RAM is available</li> <li>Close other memory-intensive applications</li> <li>For M1/M2 Macs, check Activity Monitor for memory pressure</li> </ul>"},{"location":"providers/ollama/#macos-specific-optimization","title":"MacOS-Specific Optimization","text":"<p>To get the best performance with Ollama on macOS:</p> <ul> <li>Ensure the latest version of macOS is installed</li> <li>Keep Ollama updated for latest Metal API optimizations</li> <li>For Apple Silicon Macs, use models optimized for Metal (e.g., llama2:7b-q4_0)</li> <li>Oblix's ResourceMonitor will automatically detect Metal compatibility</li> </ul>"},{"location":"providers/ollama/#api-reference","title":"API Reference","text":"<p>See the OblixClient API reference for detailed information on all available methods.</p>"},{"location":"providers/openai/","title":"OpenAI","text":"<p>OpenAI is a leading AI research laboratory that provides a range of powerful language models through its API. This page covers how to use OpenAI models with Oblix for intelligent model orchestration.</p>"},{"location":"providers/openai/#installation-and-setup","title":"Installation and Setup","text":"<p>Oblix is available exclusively for macOS at this time. To install:</p> <ol> <li>Visit oblix.ai to download the latest version</li> <li>Follow the installation instructions on the website</li> <li>Once installed, you can import the SDK in your Python projects</li> </ol> <p>Note: Currently, only macOS is supported. Windows and Linux versions are planned for future releases.</p> <p>You'll need an OpenAI API key to use OpenAI models. You can get one from the OpenAI API Dashboard.</p>"},{"location":"providers/openai/#hooking-openai-models","title":"Hooking OpenAI Models","text":"<pre><code>from oblix import OblixClient, ModelType\n\n# Initialize client\nclient = OblixClient(oblix_api_key=\"your_oblix_api_key\")\n\n# Hook OpenAI model\nawait client.hook_model(\n    model_type=ModelType.OPENAI,\n    model_name=\"gpt-3.5-turbo\",\n    api_key=\"your_openai_api_key\"\n)\n</code></pre>"},{"location":"providers/openai/#supported-openai-models","title":"Supported OpenAI Models","text":"<p>Oblix supports most OpenAI models, including:</p>"},{"location":"providers/openai/#gpt-4-series","title":"GPT-4 Series","text":"<ul> <li><code>gpt-4-turbo</code>: The most capable and up-to-date GPT-4 model with 128K context</li> <li><code>gpt-4</code>: Original GPT-4 model with 8K context</li> <li><code>gpt-4-vision</code>: GPT-4 model with vision capabilities</li> </ul>"},{"location":"providers/openai/#gpt-35-series","title":"GPT-3.5 Series","text":"<ul> <li><code>gpt-3.5-turbo</code>: Balanced model optimized for chat at a lower cost</li> <li><code>gpt-3.5-turbo-16k</code>: Extended context version of GPT-3.5 Turbo</li> </ul>"},{"location":"providers/openai/#using-openai-with-oblix","title":"Using OpenAI with Oblix","text":""},{"location":"providers/openai/#basic-execution","title":"Basic Execution","text":"<pre><code>from oblix import OblixClient, ModelType\n\n# Initialize client\nclient = OblixClient(oblix_api_key=\"your_oblix_api_key\")\n\n# Hook OpenAI model\nawait client.hook_model(\n    model_type=ModelType.OPENAI,\n    model_name=\"gpt-3.5-turbo\",\n    api_key=\"your_openai_api_key\"\n)\n\n# Execute prompt using OpenAI\nresponse = await client.execute(\n    \"Explain quantum computing in simple terms\",\n    model_id=\"openai:gpt-3.5-turbo\"  # Optionally specify model\n)\n\nprint(response[\"response\"])\n</code></pre>"},{"location":"providers/openai/#hybrid-local-cloud-setup-with-openai","title":"Hybrid Local-Cloud Setup with OpenAI","text":"<pre><code>from oblix import OblixClient, ModelType\nfrom oblix.agents.resource_monitor import ResourceMonitor\nfrom oblix.agents.connectivity import ConnectivityAgent\n\n# Initialize client\nclient = OblixClient(oblix_api_key=\"your_oblix_api_key\")\n\n# Hook local Ollama model\nawait client.hook_model(\n    model_type=ModelType.OLLAMA,\n    model_name=\"llama2\",\n    endpoint=\"http://localhost:11434\"\n)\n\n# Hook cloud OpenAI model\nawait client.hook_model(\n    model_type=ModelType.OPENAI,\n    model_name=\"gpt-3.5-turbo\",\n    api_key=\"your_openai_api_key\"\n)\n\n# Add monitoring agents for intelligent orchestration\nclient.hook_agent(ResourceMonitor())\nclient.hook_agent(ConnectivityAgent())\n\n# Execute prompt - Oblix will automatically choose between \n# local and cloud models based on connectivity and system resources\nresponse = await client.execute(\"Explain quantum computing in simple terms\")\nprint(response[\"response\"])\n</code></pre>"},{"location":"providers/openai/#advanced-configuration","title":"Advanced Configuration","text":"<p>You can customize OpenAI model settings when executing prompts:</p> <pre><code>response = await client.execute(\n    \"Write a story about a robot who gains consciousness\",\n    model_id=\"openai:gpt-4\",\n    temperature=0.8,\n    max_tokens=2000,\n    top_p=0.95\n)\n</code></pre>"},{"location":"providers/openai/#supported-parameters","title":"Supported Parameters","text":"<p>OpenAI models in Oblix support all standard OpenAI parameters:</p> Parameter Description Default <code>temperature</code> Controls randomness (0-2) 0.7 <code>max_tokens</code> Maximum response length model-dependent <code>top_p</code> Nucleus sampling parameter 1.0 <code>presence_penalty</code> Penalizes repeated tokens 0.0 <code>frequency_penalty</code> Penalizes frequent tokens 0.0"},{"location":"providers/openai/#performance-considerations","title":"Performance Considerations","text":"<p>When using OpenAI models with Oblix, consider:</p>"},{"location":"providers/openai/#cost-optimization","title":"Cost Optimization","text":"<ul> <li>Use <code>gpt-3.5-turbo</code> for routine tasks</li> <li>Reserve <code>gpt-4</code> for complex reasoning and creative tasks</li> <li>Let Oblix's orchestration system automatically route to local models when appropriate</li> </ul>"},{"location":"providers/openai/#latency-management","title":"Latency Management","text":"<ul> <li>OpenAI API calls introduce network latency</li> <li>Response time increases with model size and response length</li> <li>Oblix's connectivity monitoring helps manage latency expectations</li> </ul>"},{"location":"providers/openai/#token-counting","title":"Token Counting","text":"<ul> <li>OpenAI models have token limits per request</li> <li>Oblix tracks token usage for monitoring purposes</li> <li>Consider context length for complex conversations</li> </ul>"},{"location":"providers/openai/#macos-specific-considerations","title":"macOS Specific Considerations","text":"<ul> <li>Oblix is optimized for macOS system resource monitoring</li> <li>Ensures efficient orchestration between local and cloud models on Mac hardware</li> <li>Takes advantage of Metal-compatible GPUs when available</li> </ul>"},{"location":"providers/openai/#error-handling","title":"Error Handling","text":"<p>Oblix provides robust error handling for OpenAI API interactions:</p> <pre><code>try:\n    response = await client.execute(\"Generate a complex analysis\", model_id=\"openai:gpt-4\")\nexcept Exception as e:\n    print(f\"Error: {e}\")\n    # Oblix will automatically try to use a fallback model if available\n</code></pre>"},{"location":"providers/openai/#api-reference","title":"API Reference","text":"<p>See the OblixClient API reference for detailed information on all available methods.</p>"}]}